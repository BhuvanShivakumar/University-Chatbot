{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4d4752-dd7e-49d5-8f56-a4e3988641e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-large-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b73dde-5b80-4512-895b-1c1b8f726503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27ba30cc-5df1-42e2-9ff3-7b9f8749aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess your dataset\n",
    "with open('Combined_training.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract patterns and responses\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        questions.append(pattern)\n",
    "        labels.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2a85129-0e9f-44f4-82eb-3db20f718df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "MODEL_TYPE = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a25c918-0a11-4bf7-9bdf-8863d2702a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_data(questions, max_length):\n",
    "    encodings = tokenizer(questions, truncation=True, padding=True, max_length=max_length)\n",
    "    return encodings\n",
    "\n",
    "# Convert data to TensorFlow Dataset format\n",
    "def convert_to_tf_dataset(encodings, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\n",
    "    dataset = dataset.shuffle(len(labels)).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ef5b55d-4a16-4eaf-861a-3ec67d1ab928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 10\n",
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5f137c-da38-4f62-9462-39827e496b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 21s 4s/step - loss: 2.8744 - accuracy: 0.0714 - val_loss: 2.8005 - val_accuracy: 0.1429\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 8s 3s/step - loss: 2.7952 - accuracy: 0.1071 - val_loss: 2.5837 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 8s 3s/step - loss: 2.5263 - accuracy: 0.2500 - val_loss: 2.2374 - val_accuracy: 0.4286\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 8s 3s/step - loss: 2.2314 - accuracy: 0.4048 - val_loss: 2.0012 - val_accuracy: 0.5714\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.0431 - accuracy: 0.5714 - val_loss: 1.7917 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.8620 - accuracy: 0.7381 - val_loss: 1.5416 - val_accuracy: 0.9048\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.6122 - accuracy: 0.8214 - val_loss: 1.4327 - val_accuracy: 0.8571\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.3005 - accuracy: 0.9167 - val_loss: 1.1422 - val_accuracy: 0.9524\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.0941 - accuracy: 0.9762 - val_loss: 1.0055 - val_accuracy: 0.9524\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.9238 - accuracy: 0.9524 - val_loss: 0.8295 - val_accuracy: 0.9524\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x39ab02670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x39ab02670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Fold 1 results - Accuracy: 9.52%, Precision: 7.62%, Recall: 9.52%, F1 Score: 8.47%\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 20s 4s/step - loss: 2.9203 - accuracy: 0.0357 - val_loss: 2.7853 - val_accuracy: 0.0476\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.7340 - accuracy: 0.1310 - val_loss: 2.8012 - val_accuracy: 0.1905\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 10s 3s/step - loss: 2.5687 - accuracy: 0.1786 - val_loss: 2.7835 - val_accuracy: 0.0476\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.4223 - accuracy: 0.2738 - val_loss: 2.5673 - val_accuracy: 0.2381\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 10s 3s/step - loss: 2.2875 - accuracy: 0.4048 - val_loss: 2.5224 - val_accuracy: 0.2381\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.0540 - accuracy: 0.5119 - val_loss: 2.1851 - val_accuracy: 0.3810\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.7414 - accuracy: 0.6071 - val_loss: 1.9166 - val_accuracy: 0.6190\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 10s 3s/step - loss: 1.4756 - accuracy: 0.8214 - val_loss: 1.5270 - val_accuracy: 0.7619\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.2360 - accuracy: 0.9286 - val_loss: 1.2763 - val_accuracy: 0.8571\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.9481 - accuracy: 0.9881 - val_loss: 1.0211 - val_accuracy: 0.8571\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Fold 2 results - Accuracy: 4.76%, Precision: 4.76%, Recall: 4.76%, F1 Score: 4.76%\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 19s 4s/step - loss: 2.8250 - accuracy: 0.0595 - val_loss: 2.7676 - val_accuracy: 0.0476\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.6653 - accuracy: 0.1548 - val_loss: 2.6157 - val_accuracy: 0.1905\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.5628 - accuracy: 0.2381 - val_loss: 2.3262 - val_accuracy: 0.3333\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.3181 - accuracy: 0.2738 - val_loss: 2.1059 - val_accuracy: 0.2857\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.1276 - accuracy: 0.3690 - val_loss: 1.8644 - val_accuracy: 0.2857\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.0002 - accuracy: 0.3810 - val_loss: 1.6550 - val_accuracy: 0.5238\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.7334 - accuracy: 0.6310 - val_loss: 1.5124 - val_accuracy: 0.6667\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.5918 - accuracy: 0.6548 - val_loss: 1.2698 - val_accuracy: 0.6667\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.4903 - accuracy: 0.7024 - val_loss: 1.1865 - val_accuracy: 0.6667\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.3459 - accuracy: 0.7976 - val_loss: 0.9806 - val_accuracy: 0.7143\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "Fold 3 results - Accuracy: 14.29%, Precision: 11.90%, Recall: 14.29%, F1 Score: 12.70%\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 18s 3s/step - loss: 2.8293 - accuracy: 0.0714 - val_loss: 2.7956 - val_accuracy: 0.0952\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.4903 - accuracy: 0.2976 - val_loss: 2.7473 - val_accuracy: 0.0952\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.2411 - accuracy: 0.3571 - val_loss: 2.7443 - val_accuracy: 0.0952\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.0798 - accuracy: 0.4048 - val_loss: 2.6481 - val_accuracy: 0.1905\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.9286 - accuracy: 0.4643 - val_loss: 2.4441 - val_accuracy: 0.2857\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.8531 - accuracy: 0.4762 - val_loss: 2.3256 - val_accuracy: 0.3810\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.6286 - accuracy: 0.6667 - val_loss: 2.1383 - val_accuracy: 0.4286\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.4927 - accuracy: 0.7381 - val_loss: 1.9689 - val_accuracy: 0.4762\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.2996 - accuracy: 0.7976 - val_loss: 1.7418 - val_accuracy: 0.5238\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.1399 - accuracy: 0.8452 - val_loss: 1.5395 - val_accuracy: 0.6190\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Fold 4 results - Accuracy: 9.52%, Precision: 5.44%, Recall: 9.52%, F1 Score: 5.95%\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 19s 4s/step - loss: 2.8265 - accuracy: 0.0833 - val_loss: 2.7400 - val_accuracy: 0.0476\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.5810 - accuracy: 0.1310 - val_loss: 2.5102 - val_accuracy: 0.1905\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.4123 - accuracy: 0.3095 - val_loss: 2.2498 - val_accuracy: 0.3333\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.1981 - accuracy: 0.4405 - val_loss: 2.1210 - val_accuracy: 0.6190\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.0297 - accuracy: 0.6071 - val_loss: 1.8773 - val_accuracy: 0.7619\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.8130 - accuracy: 0.7619 - val_loss: 1.6719 - val_accuracy: 0.8571\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.5412 - accuracy: 0.7976 - val_loss: 1.4072 - val_accuracy: 0.9524\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.3339 - accuracy: 0.8810 - val_loss: 1.1652 - val_accuracy: 0.9524\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.0778 - accuracy: 0.9524 - val_loss: 0.9618 - val_accuracy: 0.9524\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.8176 - accuracy: 0.9643 - val_loss: 0.7948 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "Fold 5 results - Accuracy: 0.00%, Precision: 0.00%, Recall: 0.00%, F1 Score: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "results = []\n",
    "\n",
    "training_times = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for train_index, test_index in kf.split(questions):\n",
    "    fold += 1\n",
    "    print(f\"Fold {fold}\")\n",
    "\n",
    "    train_questions = [questions[i] for i in train_index]\n",
    "    test_questions = [questions[i] for i in test_index]\n",
    "    train_labels = [encoded_labels[i] for i in train_index]\n",
    "    test_labels = [encoded_labels[i] for i in test_index]\n",
    "\n",
    "    train_encodings = tokenize_data(train_questions, max_length=MAX_LENGTH)\n",
    "    test_encodings = tokenize_data(test_questions, max_length=MAX_LENGTH)\n",
    "\n",
    "    train_dataset = convert_to_tf_dataset(train_encodings, train_labels, batch_size=BATCH_SIZE)\n",
    "    test_dataset = convert_to_tf_dataset(test_encodings, test_labels, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Load BERT model for sequence classification\n",
    "    model = TFBertForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset, verbose=1)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    training_times.append(training_time)\n",
    "    training_losses.append(np.mean(history.history['loss']))\n",
    "    validation_losses.append(np.mean(history.history['val_loss']))\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(test_dataset).logits\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions) * 100\n",
    "    precision = precision_score(test_labels, predictions, average='weighted') * 100\n",
    "    recall = recall_score(test_labels, predictions, average='weighted') * 100\n",
    "    f1 = f1_score(test_labels, predictions, average='weighted') * 100\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {fold} results - Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}%, Recall: {recall:.2f}%, F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5afea0cd-b4b7-4c70-be75-ab215438d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average results across all folds:\n",
      "Accuracy: 7.62%\n",
      "Precision: 5.95%\n",
      "Recall: 7.62%\n",
      "F1: 6.38%\n",
      "Average Training Time: 100.02 seconds\n",
      "Average Training Loss: 1.9585\n",
      "Average Validation Loss: 1.9526\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print average metrics\n",
    "average_results = {metric: np.mean([result[metric] for result in results]) for metric in results[0].keys() if metric != 'fold'}\n",
    "average_training_time = np.mean(training_times)\n",
    "average_training_loss = np.mean(training_losses)\n",
    "average_validation_loss = np.mean(validation_losses)\n",
    "\n",
    "print(\"\\nAverage results across all folds:\")\n",
    "for metric, value in average_results.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.2f}%\")\n",
    "\n",
    "print(f\"Average Training Time: {average_training_time:.2f} seconds\")\n",
    "print(f\"Average Training Loss: {average_training_loss:.4f}\")\n",
    "print(f\"Average Validation Loss: {average_validation_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c619688-a814-40dc-b421-788e66f9defa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert_chatbot_model were not used when initializing TFBertForSequenceClassification: ['dropout_739']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert_chatbot_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Save the final model, tokenizer, and label encoder\n",
    "model.save_pretrained('bert_chatbot_model')\n",
    "tokenizer.save_pretrained('bert_chatbot_tokenizer')\n",
    "np.save('label_encoder_classes.npy', label_encoder.classes_)\n",
    "\n",
    "# Load the model, tokenizer, and label encoder\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert_chatbot_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert_chatbot_tokenizer')\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f133d2a-f98a-4498-b5f8-19bb052ccbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'exit' to stop)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Good day! What do you need help with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset again for responses\n",
    "with open('Combined_training.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a dictionary to map tags to responses\n",
    "tag_to_response = {intent['tag']: intent['responses'] for intent in data['intents']}\n",
    "\n",
    "# Function to chat with the model\n",
    "def chat_with_bert(user_input):\n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(user_input, return_tensors='tf', max_length=128, padding='max_length', truncation=True)\n",
    "    \n",
    "    # Predict the response\n",
    "    outputs = model(inputs)\n",
    "    predicted_label = tf.argmax(outputs.logits, axis=1).numpy()[0]\n",
    "    predicted_tag = label_encoder.inverse_transform([predicted_label])[0]\n",
    "    \n",
    "    # Get the response from the tag_to_response dictionary\n",
    "    if predicted_tag in tag_to_response:\n",
    "        predicted_response = np.random.choice(tag_to_response[predicted_tag])\n",
    "    else:\n",
    "        predicted_response = \"I'm not sure how to respond to that. Can you please rephrase?\"\n",
    "\n",
    "    return predicted_response\n",
    "\n",
    "# Interactive chat\n",
    "print(\"Start chatting with the bot (type 'exit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = chat_with_bert(user_input)\n",
    "    print(f\"Chatbot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5db6b-98ba-4798-8c55-a471934dc004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e490ea-90ff-429d-9e45-d95a51d16f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5c5ca-94e7-43a7-9040-d5bdb0e17875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda1cfe-4bda-483b-b398-ea6f6c73cb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4822a9b-f230-4d74-840c-844991f4ad6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9913dfbb-a987-43e0-abcc-abcf55e8988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffef5e28-bcd7-49ce-8e92-19fb2df25b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11329d45-0e1a-4afd-94bc-ae6ebc202896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess your dataset\n",
    "with open('Combined_training.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract patterns and responses\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        questions.append(pattern)\n",
    "        labels.append(intent['tag'])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afe165b1-02d3-4ebf-a31c-bb95630d2feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load DistilBERT tokenizer and model\n",
    "MODEL_TYPE = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels=len(set(labels)))\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(questions, max_length=128):\n",
    "    return tokenizer(questions, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert data to TensorFlow Dataset format\n",
    "def convert_to_tf_dataset(encodings, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\n",
    "    dataset = dataset.shuffle(len(labels)).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a086529-fa84-4154-a4c4-b0dfc096b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 10\n",
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3b6db2b-3996-44e8-8acc-08c9d8a1711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 7s 1s/step - loss: 2.8923 - accuracy: 0.0952 - val_loss: 2.8082 - val_accuracy: 0.1905\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 2s 787ms/step - loss: 2.7251 - accuracy: 0.1190 - val_loss: 2.7026 - val_accuracy: 0.1905\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 2s 790ms/step - loss: 2.5764 - accuracy: 0.2262 - val_loss: 2.4759 - val_accuracy: 0.1905\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 2s 803ms/step - loss: 2.3954 - accuracy: 0.2262 - val_loss: 2.2032 - val_accuracy: 0.1905\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 2s 812ms/step - loss: 2.1309 - accuracy: 0.3095 - val_loss: 2.0410 - val_accuracy: 0.2857\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 2s 817ms/step - loss: 1.8873 - accuracy: 0.6548 - val_loss: 1.8139 - val_accuracy: 0.5238\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 852ms/step - loss: 1.5858 - accuracy: 0.7024 - val_loss: 1.6292 - val_accuracy: 0.4762\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 822ms/step - loss: 1.2606 - accuracy: 0.8095 - val_loss: 1.1897 - val_accuracy: 0.7143\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 824ms/step - loss: 0.9453 - accuracy: 0.8929 - val_loss: 1.0069 - val_accuracy: 0.8095\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 824ms/step - loss: 0.7104 - accuracy: 0.9762 - val_loss: 0.7233 - val_accuracy: 0.8571\n",
      "1/1 [==============================] - 1s 895ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results - Accuracy: 4.76%, Precision: 4.76%, Recall: 4.76%, F1 Score: 4.76%\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 7s 1s/step - loss: 2.9351 - accuracy: 0.1071 - val_loss: 2.9924 - val_accuracy: 0.1429\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 834ms/step - loss: 2.6950 - accuracy: 0.1786 - val_loss: 2.9981 - val_accuracy: 0.1429\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 826ms/step - loss: 2.6310 - accuracy: 0.1667 - val_loss: 2.8368 - val_accuracy: 0.1429\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 830ms/step - loss: 2.5620 - accuracy: 0.1548 - val_loss: 2.6363 - val_accuracy: 0.1429\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 853ms/step - loss: 2.2787 - accuracy: 0.3571 - val_loss: 2.3978 - val_accuracy: 0.2381\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 858ms/step - loss: 2.0377 - accuracy: 0.3571 - val_loss: 2.2459 - val_accuracy: 0.2857\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 886ms/step - loss: 1.7266 - accuracy: 0.6310 - val_loss: 1.9502 - val_accuracy: 0.4286\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 843ms/step - loss: 1.4521 - accuracy: 0.8095 - val_loss: 1.5859 - val_accuracy: 0.5238\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 838ms/step - loss: 1.0994 - accuracy: 0.8929 - val_loss: 1.2408 - val_accuracy: 0.6190\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 855ms/step - loss: 0.8483 - accuracy: 0.9762 - val_loss: 0.9485 - val_accuracy: 0.7143\n",
      "1/1 [==============================] - 1s 941ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results - Accuracy: 9.52%, Precision: 9.52%, Recall: 9.52%, F1 Score: 9.52%\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 8s 1s/step - loss: 2.9109 - accuracy: 0.0595 - val_loss: 2.7885 - val_accuracy: 0.2381\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 844ms/step - loss: 2.6509 - accuracy: 0.1190 - val_loss: 2.6597 - val_accuracy: 0.2381\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 858ms/step - loss: 2.5494 - accuracy: 0.2143 - val_loss: 2.5205 - val_accuracy: 0.2381\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 871ms/step - loss: 2.3065 - accuracy: 0.3214 - val_loss: 2.2262 - val_accuracy: 0.3810\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 866ms/step - loss: 2.0821 - accuracy: 0.4881 - val_loss: 2.1106 - val_accuracy: 0.3810\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 874ms/step - loss: 1.7675 - accuracy: 0.6667 - val_loss: 1.9843 - val_accuracy: 0.2381\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 879ms/step - loss: 1.4888 - accuracy: 0.7143 - val_loss: 1.6436 - val_accuracy: 0.5238\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 872ms/step - loss: 1.1613 - accuracy: 0.8810 - val_loss: 1.4011 - val_accuracy: 0.7619\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 886ms/step - loss: 0.9261 - accuracy: 0.9167 - val_loss: 1.2130 - val_accuracy: 0.7619\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 894ms/step - loss: 0.7019 - accuracy: 0.9643 - val_loss: 1.0512 - val_accuracy: 0.6667\n",
      "1/1 [==============================] - 1s 940ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results - Accuracy: 9.52%, Precision: 7.14%, Recall: 9.52%, F1 Score: 7.94%\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 7s 1s/step - loss: 2.9468 - accuracy: 0.1071 - val_loss: 3.2259 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 847ms/step - loss: 2.6192 - accuracy: 0.1786 - val_loss: 3.0225 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 848ms/step - loss: 2.5808 - accuracy: 0.2500 - val_loss: 2.9803 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 848ms/step - loss: 2.3867 - accuracy: 0.2262 - val_loss: 2.8707 - val_accuracy: 0.1429\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 854ms/step - loss: 2.2012 - accuracy: 0.3810 - val_loss: 2.7233 - val_accuracy: 0.1905\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 854ms/step - loss: 1.9510 - accuracy: 0.4881 - val_loss: 2.4648 - val_accuracy: 0.1905\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 954ms/step - loss: 1.7445 - accuracy: 0.6190 - val_loss: 2.3336 - val_accuracy: 0.3333\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 872ms/step - loss: 1.4462 - accuracy: 0.7381 - val_loss: 1.9805 - val_accuracy: 0.3333\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 877ms/step - loss: 1.1872 - accuracy: 0.8452 - val_loss: 1.7365 - val_accuracy: 0.5714\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 865ms/step - loss: 0.9574 - accuracy: 0.8929 - val_loss: 1.4605 - val_accuracy: 0.5714\n",
      "1/1 [==============================] - 1s 905ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results - Accuracy: 0.00%, Precision: 0.00%, Recall: 0.00%, F1 Score: 0.00%\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 7s 1s/step - loss: 2.9323 - accuracy: 0.1190 - val_loss: 2.8315 - val_accuracy: 0.1429\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 878ms/step - loss: 2.7036 - accuracy: 0.1548 - val_loss: 2.8340 - val_accuracy: 0.1429\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 862ms/step - loss: 2.5790 - accuracy: 0.2381 - val_loss: 2.6738 - val_accuracy: 0.1429\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 860ms/step - loss: 2.4489 - accuracy: 0.2381 - val_loss: 2.3749 - val_accuracy: 0.2381\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 863ms/step - loss: 2.1681 - accuracy: 0.4762 - val_loss: 2.1669 - val_accuracy: 0.3333\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 919ms/step - loss: 1.9348 - accuracy: 0.5000 - val_loss: 1.9447 - val_accuracy: 0.4286\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 878ms/step - loss: 1.6869 - accuracy: 0.6429 - val_loss: 1.7072 - val_accuracy: 0.7143\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 866ms/step - loss: 1.3488 - accuracy: 0.7976 - val_loss: 1.3737 - val_accuracy: 0.7143\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 872ms/step - loss: 1.0549 - accuracy: 0.9524 - val_loss: 1.1518 - val_accuracy: 0.7619\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 884ms/step - loss: 0.8222 - accuracy: 0.9405 - val_loss: 0.8524 - val_accuracy: 0.9048\n",
      "1/1 [==============================] - 1s 962ms/step\n",
      "Fold 5 results - Accuracy: 4.76%, Precision: 4.76%, Recall: 4.76%, F1 Score: 4.76%\n"
     ]
    }
   ],
   "source": [
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "results = []\n",
    "\n",
    "training_times = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for train_index, test_index in kf.split(questions):\n",
    "    fold += 1\n",
    "    print(f\"Fold {fold}\")\n",
    "\n",
    "    train_questions = [questions[i] for i in train_index]\n",
    "    test_questions = [questions[i] for i in test_index]\n",
    "    train_labels = [encoded_labels[i] for i in train_index]\n",
    "    test_labels = [encoded_labels[i] for i in test_index]\n",
    "\n",
    "    train_encodings = tokenize_data(train_questions, max_length=MAX_LENGTH)\n",
    "    test_encodings = tokenize_data(test_questions, max_length=MAX_LENGTH)\n",
    "\n",
    "    train_dataset = convert_to_tf_dataset(train_encodings, train_labels, batch_size=BATCH_SIZE)\n",
    "    test_dataset = convert_to_tf_dataset(test_encodings, test_labels, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Load BERT model for sequence classification\n",
    "    model = TFBertForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset, verbose=1)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    training_times.append(training_time)\n",
    "    training_losses.append(np.mean(history.history['loss']))\n",
    "    validation_losses.append(np.mean(history.history['val_loss']))\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(test_dataset).logits\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions) * 100\n",
    "    precision = precision_score(test_labels, predictions, average='weighted') * 100\n",
    "    recall = recall_score(test_labels, predictions, average='weighted') * 100\n",
    "    f1 = f1_score(test_labels, predictions, average='weighted') * 100\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {fold} results - Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}%, Recall: {recall:.2f}%, F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cb2b00d-8bda-4020-934a-bee46f8b511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average results across all folds:\n",
      "Accuracy: 5.71%\n",
      "Precision: 5.24%\n",
      "Recall: 5.71%\n",
      "F1: 5.40%\n",
      "Average Training Time: 30.91 seconds\n",
      "Average Training Loss: 1.9524\n",
      "Average Validation Loss: 2.0947\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print average metrics\n",
    "average_results = {metric: np.mean([result[metric] for result in results]) for metric in results[0].keys() if metric != 'fold'}\n",
    "average_training_time = np.mean(training_times)\n",
    "average_training_loss = np.mean(training_losses)\n",
    "average_validation_loss = np.mean(validation_losses)\n",
    "\n",
    "print(\"\\nAverage results across all folds:\")\n",
    "for metric, value in average_results.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.2f}%\")\n",
    "\n",
    "print(f\"Average Training Time: {average_training_time:.2f} seconds\")\n",
    "print(f\"Average Training Loss: {average_training_loss:.4f}\")\n",
    "print(f\"Average Validation Loss: {average_validation_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f379df1-fc3a-4e14-8b2b-2c408eb7f9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert_chatbot_model were not used when initializing TFBertForSequenceClassification: ['dropout_1023']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert_chatbot_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Save the final model, tokenizer, and label encoder\n",
    "model.save_pretrained('bert_chatbot_model')\n",
    "tokenizer.save_pretrained('bert_chatbot_tokenizer')\n",
    "np.save('label_encoder_classes.npy', label_encoder.classes_)\n",
    "\n",
    "# Load the model, tokenizer, and label encoder\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert_chatbot_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert_chatbot_tokenizer')\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "392c1eac-c191-4449-9530-22b8cd144a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'exit' to stop)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset again for responses\n",
    "with open('Combined_training.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a dictionary to map tags to responses\n",
    "tag_to_response = {intent['tag']: intent['responses'] for intent in data['intents']}\n",
    "\n",
    "# Function to chat with the model\n",
    "def chat_with_bert(user_input):\n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(user_input, return_tensors='tf', max_length=128, padding='max_length', truncation=True)\n",
    "    \n",
    "    # Predict the response\n",
    "    outputs = model(inputs)\n",
    "    predicted_label = tf.argmax(outputs.logits, axis=1).numpy()[0]\n",
    "    predicted_tag = label_encoder.inverse_transform([predicted_label])[0]\n",
    "    \n",
    "    # Get the response from the tag_to_response dictionary\n",
    "    if predicted_tag in tag_to_response:\n",
    "        predicted_response = np.random.choice(tag_to_response[predicted_tag])\n",
    "    else:\n",
    "        predicted_response = \"I'm not sure how to respond to that. Can you please rephrase?\"\n",
    "\n",
    "    return predicted_response\n",
    "\n",
    "# Interactive chat\n",
    "print(\"Start chatting with the bot (type 'exit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = chat_with_bert(user_input)\n",
    "    print(f\"Chatbot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff9b1d-1c46-47b6-898d-632450a475e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e31cf48f-9649-4808-bbd3-fc6f835960e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "111360c3-4765-44a6-b67e-e812f954b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a506719-5c99-4ef4-926b-014e7f2facf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess your dataset\n",
    "with open('Combined_training.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract patterns and responses\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        questions.append(pattern)\n",
    "        labels.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b9822fe-9608-4d64-b2c5-7642b6889aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac38357e-408e-44c7-b2a0-587892d0e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_data(questions, max_length):\n",
    "    encodings = tokenizer(questions, truncation=True, padding=True, max_length=max_length)\n",
    "    return encodings\n",
    "\n",
    "# Convert data to TensorFlow Dataset format\n",
    "def convert_to_tf_dataset(encodings, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\n",
    "    dataset = dataset.shuffle(len(labels)).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55a792ff-b626-4690-beda-796a94753dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 10\n",
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "992d54b7-1e65-4720-a4ed-3758bcdf4fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 7s 1s/step - loss: 2.7948 - accuracy: 0.0595 - val_loss: 2.6217 - val_accuracy: 0.1905\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 2s 793ms/step - loss: 2.6356 - accuracy: 0.1310 - val_loss: 2.4861 - val_accuracy: 0.2857\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 2s 793ms/step - loss: 2.4779 - accuracy: 0.2262 - val_loss: 2.3382 - val_accuracy: 0.4286\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 2s 808ms/step - loss: 2.3449 - accuracy: 0.3810 - val_loss: 2.2655 - val_accuracy: 0.4286\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 2s 807ms/step - loss: 2.1861 - accuracy: 0.4881 - val_loss: 2.1421 - val_accuracy: 0.4286\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 839ms/step - loss: 1.9866 - accuracy: 0.6071 - val_loss: 1.8993 - val_accuracy: 0.6190\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 826ms/step - loss: 1.8484 - accuracy: 0.7381 - val_loss: 1.7358 - val_accuracy: 0.6190\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 861ms/step - loss: 1.6858 - accuracy: 0.8095 - val_loss: 1.5657 - val_accuracy: 0.7619\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 839ms/step - loss: 1.5390 - accuracy: 0.8571 - val_loss: 1.4256 - val_accuracy: 0.7619\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 843ms/step - loss: 1.3674 - accuracy: 0.8810 - val_loss: 1.3245 - val_accuracy: 0.8095\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Fold 1 results - Accuracy: 4.76%, Precision: 4.76%, Recall: 4.76%, F1 Score: 4.76%\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 7s 1s/step - loss: 2.7817 - accuracy: 0.1310 - val_loss: 2.8242 - val_accuracy: 0.1429\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 831ms/step - loss: 2.6177 - accuracy: 0.1548 - val_loss: 2.6144 - val_accuracy: 0.2381\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 835ms/step - loss: 2.4445 - accuracy: 0.3333 - val_loss: 2.3664 - val_accuracy: 0.3810\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 839ms/step - loss: 2.2648 - accuracy: 0.4286 - val_loss: 2.1245 - val_accuracy: 0.5238\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 839ms/step - loss: 2.0868 - accuracy: 0.5952 - val_loss: 1.9445 - val_accuracy: 0.7619\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 855ms/step - loss: 1.8985 - accuracy: 0.6905 - val_loss: 1.7764 - val_accuracy: 0.8095\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 851ms/step - loss: 1.7038 - accuracy: 0.7976 - val_loss: 1.6792 - val_accuracy: 0.9524\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 900ms/step - loss: 1.5287 - accuracy: 0.9405 - val_loss: 1.5484 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 853ms/step - loss: 1.3797 - accuracy: 0.9881 - val_loss: 1.3953 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 856ms/step - loss: 1.2113 - accuracy: 0.9881 - val_loss: 1.2566 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Fold 2 results - Accuracy: 4.76%, Precision: 4.76%, Recall: 4.76%, F1 Score: 4.76%\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 7s 1s/step - loss: 2.7892 - accuracy: 0.0833 - val_loss: 2.7056 - val_accuracy: 0.1905\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 854ms/step - loss: 2.6832 - accuracy: 0.1548 - val_loss: 2.6347 - val_accuracy: 0.2381\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 854ms/step - loss: 2.5564 - accuracy: 0.2262 - val_loss: 2.6294 - val_accuracy: 0.2857\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 863ms/step - loss: 2.4637 - accuracy: 0.2857 - val_loss: 2.5912 - val_accuracy: 0.2381\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 866ms/step - loss: 2.3704 - accuracy: 0.2500 - val_loss: 2.4974 - val_accuracy: 0.2857\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 866ms/step - loss: 2.2866 - accuracy: 0.2738 - val_loss: 2.4440 - val_accuracy: 0.2857\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 863ms/step - loss: 2.1242 - accuracy: 0.4524 - val_loss: 2.3159 - val_accuracy: 0.3333\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 864ms/step - loss: 2.0184 - accuracy: 0.5000 - val_loss: 2.2410 - val_accuracy: 0.3333\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 868ms/step - loss: 1.8382 - accuracy: 0.6310 - val_loss: 2.0748 - val_accuracy: 0.5238\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 867ms/step - loss: 1.6852 - accuracy: 0.7857 - val_loss: 1.9412 - val_accuracy: 0.5238\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Fold 3 results - Accuracy: 14.29%, Precision: 12.70%, Recall: 14.29%, F1 Score: 13.42%\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 7s 1s/step - loss: 2.8254 - accuracy: 0.0357 - val_loss: 2.7613 - val_accuracy: 0.1429\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 835ms/step - loss: 2.6910 - accuracy: 0.0714 - val_loss: 2.6783 - val_accuracy: 0.1429\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 836ms/step - loss: 2.5393 - accuracy: 0.1667 - val_loss: 2.6494 - val_accuracy: 0.2381\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 853ms/step - loss: 2.4280 - accuracy: 0.2024 - val_loss: 2.4853 - val_accuracy: 0.2381\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 843ms/step - loss: 2.2766 - accuracy: 0.4286 - val_loss: 2.4272 - val_accuracy: 0.3810\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 854ms/step - loss: 2.1349 - accuracy: 0.5595 - val_loss: 2.2512 - val_accuracy: 0.3333\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 876ms/step - loss: 1.9913 - accuracy: 0.5238 - val_loss: 2.1596 - val_accuracy: 0.3810\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 848ms/step - loss: 1.8215 - accuracy: 0.5595 - val_loss: 2.0244 - val_accuracy: 0.3810\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 847ms/step - loss: 1.6448 - accuracy: 0.6667 - val_loss: 1.8670 - val_accuracy: 0.6190\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 856ms/step - loss: 1.4958 - accuracy: 0.7024 - val_loss: 1.6869 - val_accuracy: 0.7619\n",
      "1/1 [==============================] - 1s 917ms/step\n",
      "Fold 4 results - Accuracy: 14.29%, Precision: 11.11%, Recall: 14.29%, F1 Score: 11.90%\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 7s 1s/step - loss: 2.8296 - accuracy: 0.0476 - val_loss: 2.7872 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 854ms/step - loss: 2.6041 - accuracy: 0.1905 - val_loss: 2.5424 - val_accuracy: 0.1905\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 857ms/step - loss: 2.5192 - accuracy: 0.2857 - val_loss: 2.4189 - val_accuracy: 0.2857\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 859ms/step - loss: 2.3045 - accuracy: 0.4762 - val_loss: 2.2794 - val_accuracy: 0.4762\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 866ms/step - loss: 2.1535 - accuracy: 0.6310 - val_loss: 2.1355 - val_accuracy: 0.5714\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 870ms/step - loss: 2.0381 - accuracy: 0.6786 - val_loss: 1.9948 - val_accuracy: 0.6190\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 911ms/step - loss: 1.8483 - accuracy: 0.7500 - val_loss: 1.8013 - val_accuracy: 0.7619\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 870ms/step - loss: 1.6838 - accuracy: 0.9048 - val_loss: 1.6223 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 868ms/step - loss: 1.5338 - accuracy: 0.9881 - val_loss: 1.4475 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 872ms/step - loss: 1.3730 - accuracy: 0.9524 - val_loss: 1.3037 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Fold 5 results - Accuracy: 0.00%, Precision: 0.00%, Recall: 0.00%, F1 Score: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "results = []\n",
    "\n",
    "training_times = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for train_index, test_index in kf.split(questions):\n",
    "    fold += 1\n",
    "    print(f\"Fold {fold}\")\n",
    "\n",
    "    train_questions = [questions[i] for i in train_index]\n",
    "    test_questions = [questions[i] for i in test_index]\n",
    "    train_labels = [encoded_labels[i] for i in train_index]\n",
    "    test_labels = [encoded_labels[i] for i in test_index]\n",
    "\n",
    "    train_encodings = tokenize_data(train_questions, max_length=MAX_LENGTH)\n",
    "    test_encodings = tokenize_data(test_questions, max_length=MAX_LENGTH)\n",
    "\n",
    "    train_dataset = convert_to_tf_dataset(train_encodings, train_labels, batch_size=BATCH_SIZE)\n",
    "    test_dataset = convert_to_tf_dataset(test_encodings, test_labels, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Load BERT model for sequence classification\n",
    "    model = TFBertForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset, verbose=1)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    training_times.append(training_time)\n",
    "    training_losses.append(np.mean(history.history['loss']))\n",
    "    validation_losses.append(np.mean(history.history['val_loss']))\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(test_dataset).logits\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions) * 100\n",
    "    precision = precision_score(test_labels, predictions, average='weighted') * 100\n",
    "    recall = recall_score(test_labels, predictions, average='weighted') * 100\n",
    "    f1 = f1_score(test_labels, predictions, average='weighted') * 100\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {fold} results - Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}%, Recall: {recall:.2f}%, F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "792a864f-b836-4ad3-812e-3097cea60e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average results across all folds:\n",
      "Accuracy: 7.62%\n",
      "Precision: 6.67%\n",
      "Recall: 7.62%\n",
      "F1: 6.97%\n",
      "Average Training Time: 30.55 seconds\n",
      "Average Training Loss: 2.1267\n",
      "Average Validation Loss: 2.1347\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print average metrics\n",
    "average_results = {metric: np.mean([result[metric] for result in results]) for metric in results[0].keys() if metric != 'fold'}\n",
    "average_training_time = np.mean(training_times)\n",
    "average_training_loss = np.mean(training_losses)\n",
    "average_validation_loss = np.mean(validation_losses)\n",
    "\n",
    "print(\"\\nAverage results across all folds:\")\n",
    "for metric, value in average_results.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.2f}%\")\n",
    "\n",
    "print(f\"Average Training Time: {average_training_time:.2f} seconds\")\n",
    "print(f\"Average Training Loss: {average_training_loss:.4f}\")\n",
    "print(f\"Average Validation Loss: {average_validation_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e34775c4-d92f-45e6-912b-1adfdaff531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert_chatbot_model were not used when initializing TFBertForSequenceClassification: ['dropout_1251']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert_chatbot_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Save the final model, tokenizer, and label encoder\n",
    "model.save_pretrained('bert_chatbot_model')\n",
    "tokenizer.save_pretrained('bert_chatbot_tokenizer')\n",
    "np.save('label_encoder_classes.npy', label_encoder.classes_)\n",
    "\n",
    "# Load the model, tokenizer, and label encoder\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert_chatbot_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert_chatbot_tokenizer')\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45da5607-fc1d-462a-8e77-9c9283465453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'exit' to stop)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset again for responses\n",
    "with open('Combined_training.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a dictionary to map tags to responses\n",
    "tag_to_response = {intent['tag']: intent['responses'] for intent in data['intents']}\n",
    "\n",
    "# Function to chat with the model\n",
    "def chat_with_bert(user_input):\n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(user_input, return_tensors='tf', max_length=128, padding='max_length', truncation=True)\n",
    "    \n",
    "    # Predict the response\n",
    "    outputs = model(inputs)\n",
    "    predicted_label = tf.argmax(outputs.logits, axis=1).numpy()[0]\n",
    "    predicted_tag = label_encoder.inverse_transform([predicted_label])[0]\n",
    "    \n",
    "    # Get the response from the tag_to_response dictionary\n",
    "    if predicted_tag in tag_to_response:\n",
    "        predicted_response = np.random.choice(tag_to_response[predicted_tag])\n",
    "    else:\n",
    "        predicted_response = \"I'm not sure how to respond to that. Can you please rephrase?\"\n",
    "\n",
    "    return predicted_response\n",
    "\n",
    "# Interactive chat\n",
    "print(\"Start chatting with the bot (type 'exit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = chat_with_bert(user_input)\n",
    "    print(f\"Chatbot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ab3b3-2638-400c-a944-084e1b9e25f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "311b9b58-9987-4737-b00c-709da5a97ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64c6fde3-c2fc-4f2c-85c4-8baa88b77fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f49b1b1-20f2-4c5c-bf92-6929f5679b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess your dataset\n",
    "with open('Combined_training.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract patterns and responses\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        questions.append(pattern)\n",
    "        labels.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d650c25-c6e6-4870-8e8d-b94e4611eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "MODEL_TYPE = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(questions, max_length):\n",
    "    encodings = tokenizer(questions, truncation=True, padding=True, max_length=max_length)\n",
    "    return encodings\n",
    "\n",
    "# Convert data to TensorFlow Dataset format\n",
    "def convert_to_tf_dataset(encodings, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\n",
    "    dataset = dataset.shuffle(len(labels)).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c44ab70-8f1f-4434-9fcf-8e2ced190c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 10\n",
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d4c9366-1688-4ece-82be-46af0ce96a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 8s 1s/step - loss: 2.7761 - accuracy: 0.0714 - val_loss: 2.7332 - val_accuracy: 0.1905\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 842ms/step - loss: 2.7552 - accuracy: 0.1190 - val_loss: 2.7112 - val_accuracy: 0.1905\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 845ms/step - loss: 2.7073 - accuracy: 0.1310 - val_loss: 2.5921 - val_accuracy: 0.1905\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 858ms/step - loss: 2.5360 - accuracy: 0.2024 - val_loss: 2.3939 - val_accuracy: 0.2381\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 864ms/step - loss: 2.2591 - accuracy: 0.4881 - val_loss: 1.9458 - val_accuracy: 0.7619\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 870ms/step - loss: 1.9084 - accuracy: 0.8214 - val_loss: 1.5914 - val_accuracy: 0.9524\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 889ms/step - loss: 1.5648 - accuracy: 0.9643 - val_loss: 1.2549 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 879ms/step - loss: 1.2646 - accuracy: 0.9524 - val_loss: 0.9627 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 883ms/step - loss: 0.9933 - accuracy: 1.0000 - val_loss: 0.7312 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 882ms/step - loss: 0.8054 - accuracy: 1.0000 - val_loss: 0.5497 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 897ms/step\n",
      "Fold 1 results - Accuracy: 9.52%, Precision: 9.52%, Recall: 9.52%, F1 Score: 9.52%\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 7s 1s/step - loss: 2.7700 - accuracy: 0.0476 - val_loss: 2.8016 - val_accuracy: 0.1429\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 899ms/step - loss: 2.7340 - accuracy: 0.1548 - val_loss: 2.7741 - val_accuracy: 0.1429\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 889ms/step - loss: 2.6123 - accuracy: 0.2024 - val_loss: 2.6490 - val_accuracy: 0.1905\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 903ms/step - loss: 2.4051 - accuracy: 0.4286 - val_loss: 2.3584 - val_accuracy: 0.4286\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 898ms/step - loss: 2.0547 - accuracy: 0.7500 - val_loss: 1.9581 - val_accuracy: 0.7143\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 944ms/step - loss: 1.7123 - accuracy: 0.8929 - val_loss: 1.6090 - val_accuracy: 0.8095\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 907ms/step - loss: 1.3743 - accuracy: 0.9524 - val_loss: 1.2725 - val_accuracy: 0.8095\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 915ms/step - loss: 1.1040 - accuracy: 0.9762 - val_loss: 1.0179 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 920ms/step - loss: 0.8641 - accuracy: 1.0000 - val_loss: 0.8379 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 907ms/step - loss: 0.6829 - accuracy: 1.0000 - val_loss: 0.6536 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 937ms/step\n",
      "Fold 2 results - Accuracy: 9.52%, Precision: 9.52%, Recall: 9.52%, F1 Score: 9.52%\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 8s 1s/step - loss: 2.7790 - accuracy: 0.0357 - val_loss: 2.7375 - val_accuracy: 0.1429\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 895ms/step - loss: 2.7563 - accuracy: 0.1190 - val_loss: 2.7234 - val_accuracy: 0.2381\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 898ms/step - loss: 2.7079 - accuracy: 0.2381 - val_loss: 2.6654 - val_accuracy: 0.2857\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 905ms/step - loss: 2.5965 - accuracy: 0.2976 - val_loss: 2.3778 - val_accuracy: 0.3333\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 941ms/step - loss: 2.2522 - accuracy: 0.6310 - val_loss: 1.9708 - val_accuracy: 0.7619\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 912ms/step - loss: 1.8814 - accuracy: 0.8452 - val_loss: 1.6251 - val_accuracy: 0.8095\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 922ms/step - loss: 1.4985 - accuracy: 0.9762 - val_loss: 1.2895 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 924ms/step - loss: 1.2068 - accuracy: 1.0000 - val_loss: 0.9901 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 913ms/step - loss: 0.9499 - accuracy: 1.0000 - val_loss: 0.7884 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 918ms/step - loss: 0.7426 - accuracy: 1.0000 - val_loss: 0.5988 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 960ms/step\n",
      "Fold 3 results - Accuracy: 23.81%, Precision: 23.81%, Recall: 23.81%, F1 Score: 23.81%\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 8s 1s/step - loss: 2.7658 - accuracy: 0.0952 - val_loss: 2.8045 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 880ms/step - loss: 2.6914 - accuracy: 0.1667 - val_loss: 2.8149 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 893ms/step - loss: 2.6507 - accuracy: 0.1786 - val_loss: 2.7942 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 922ms/step - loss: 2.4640 - accuracy: 0.1786 - val_loss: 2.6070 - val_accuracy: 0.0952\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 896ms/step - loss: 2.1415 - accuracy: 0.5595 - val_loss: 2.2677 - val_accuracy: 0.7143\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 894ms/step - loss: 1.8082 - accuracy: 0.8095 - val_loss: 1.8446 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 909ms/step - loss: 1.5046 - accuracy: 0.9167 - val_loss: 1.5513 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 900ms/step - loss: 1.1841 - accuracy: 0.9762 - val_loss: 1.2284 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 899ms/step - loss: 0.9314 - accuracy: 0.9643 - val_loss: 0.9810 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 924ms/step - loss: 0.7625 - accuracy: 1.0000 - val_loss: 0.7544 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 928ms/step\n",
      "Fold 4 results - Accuracy: 4.76%, Precision: 4.76%, Recall: 4.76%, F1 Score: 4.76%\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 8s 1s/step - loss: 2.7809 - accuracy: 0.0357 - val_loss: 2.7748 - val_accuracy: 0.0476\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 3s 896ms/step - loss: 2.7394 - accuracy: 0.1548 - val_loss: 2.7475 - val_accuracy: 0.1429\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 903ms/step - loss: 2.6485 - accuracy: 0.1905 - val_loss: 2.5503 - val_accuracy: 0.1429\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 909ms/step - loss: 2.3552 - accuracy: 0.3333 - val_loss: 2.3132 - val_accuracy: 0.2381\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 916ms/step - loss: 2.0298 - accuracy: 0.7262 - val_loss: 1.9029 - val_accuracy: 0.8571\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 909ms/step - loss: 1.6628 - accuracy: 0.9762 - val_loss: 1.5310 - val_accuracy: 0.9048\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 921ms/step - loss: 1.3543 - accuracy: 0.9881 - val_loss: 1.2041 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 937ms/step - loss: 1.0691 - accuracy: 1.0000 - val_loss: 0.9180 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 956ms/step - loss: 0.8406 - accuracy: 1.0000 - val_loss: 0.7160 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 926ms/step - loss: 0.6619 - accuracy: 1.0000 - val_loss: 0.5491 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 964ms/step\n",
      "Fold 5 results - Accuracy: 23.81%, Precision: 23.81%, Recall: 23.81%, F1 Score: 23.81%\n"
     ]
    }
   ],
   "source": [
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "results = []\n",
    "\n",
    "training_times = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for train_index, test_index in kf.split(questions):\n",
    "    fold += 1\n",
    "    print(f\"Fold {fold}\")\n",
    "\n",
    "    train_questions = [questions[i] for i in train_index]\n",
    "    test_questions = [questions[i] for i in test_index]\n",
    "    train_labels = [encoded_labels[i] for i in train_index]\n",
    "    test_labels = [encoded_labels[i] for i in test_index]\n",
    "\n",
    "    train_encodings = tokenize_data(train_questions, max_length=MAX_LENGTH)\n",
    "    test_encodings = tokenize_data(test_questions, max_length=MAX_LENGTH)\n",
    "\n",
    "    train_dataset = convert_to_tf_dataset(train_encodings, train_labels, batch_size=BATCH_SIZE)\n",
    "    test_dataset = convert_to_tf_dataset(test_encodings, test_labels, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Load BERT model for sequence classification\n",
    "    model = TFRobertaForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset, verbose=1)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    training_times.append(training_time)\n",
    "    training_losses.append(np.mean(history.history['loss']))\n",
    "    validation_losses.append(np.mean(history.history['val_loss']))\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(test_dataset).logits\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions) * 100\n",
    "    precision = precision_score(test_labels, predictions, average='weighted') * 100\n",
    "    recall = recall_score(test_labels, predictions, average='weighted') * 100\n",
    "    f1 = f1_score(test_labels, predictions, average='weighted') * 100\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {fold} results - Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}%, Recall: {recall:.2f}%, F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afb6e028-032a-40ca-a442-35f51b7205ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average results across all folds:\n",
      "Accuracy: 14.29%\n",
      "Precision: 14.29%\n",
      "Recall: 14.29%\n",
      "F1: 14.29%\n",
      "Average Training Time: 32.41 seconds\n",
      "Average Training Loss: 1.8860\n",
      "Average Validation Loss: 1.8004\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print average metrics\n",
    "average_results = {metric: np.mean([result[metric] for result in results]) for metric in results[0].keys() if metric != 'fold'}\n",
    "average_training_time = np.mean(training_times)\n",
    "average_training_loss = np.mean(training_losses)\n",
    "average_validation_loss = np.mean(validation_losses)\n",
    "\n",
    "print(\"\\nAverage results across all folds:\")\n",
    "for metric, value in average_results.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.2f}%\")\n",
    "\n",
    "print(f\"Average Training Time: {average_training_time:.2f} seconds\")\n",
    "print(f\"Average Training Loss: {average_training_loss:.4f}\")\n",
    "print(f\"Average Validation Loss: {average_validation_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9216e1b4-6141-4b7c-bbc6-54850427533a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at roberta_chatbot_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Save the final model, tokenizer, and label encoder\n",
    "model.save_pretrained('roberta_chatbot_model')\n",
    "tokenizer.save_pretrained('roberta_chatbot_tokenizer')\n",
    "np.save('label_encoder_classes.npy', label_encoder.classes_)\n",
    "\n",
    "# Load the model, tokenizer, and label encoder\n",
    "model = TFRobertaForSequenceClassification.from_pretrained('roberta_chatbot_model')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta_chatbot_tokenizer')\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4212e6fe-2571-4000-b5b7-eb3e349349f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'exit' to stop)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset again for responses\n",
    "with open('Combined_training.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a dictionary to map tags to responses\n",
    "tag_to_response = {intent['tag']: intent['responses'] for intent in data['intents']}\n",
    "\n",
    "# Function to chat with the model\n",
    "def chat_with_bert(user_input):\n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(user_input, return_tensors='tf', max_length=128, padding='max_length', truncation=True)\n",
    "    \n",
    "    # Predict the response\n",
    "    outputs = model(inputs)\n",
    "    predicted_label = tf.argmax(outputs.logits, axis=1).numpy()[0]\n",
    "    predicted_tag = label_encoder.inverse_transform([predicted_label])[0]\n",
    "    \n",
    "    # Get the response from the tag_to_response dictionary\n",
    "    if predicted_tag in tag_to_response:\n",
    "        predicted_response = np.random.choice(tag_to_response[predicted_tag])\n",
    "    else:\n",
    "        predicted_response = \"I'm not sure how to respond to that. Can you please rephrase?\"\n",
    "\n",
    "    return predicted_response\n",
    "\n",
    "# Interactive chat\n",
    "print(\"Start chatting with the bot (type 'exit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = chat_with_bert(user_input)\n",
    "    print(f\"Chatbot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ff7ff-2b76-4b4b-bf76-f7a70361fac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fca7bf1-501a-45e2-b261-e0f2b4e79d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the plot\n",
    "models = ['roberta-base', 'bert-large-uncased', 'distilbert-base-uncased', 'bert-base-uncased']\n",
    "accuracies = [14.29, 7.62, 5.71, 7.62]\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, accuracies, color=['blue', 'orange', 'green', 'red'])\n",
    "plt.ylim(0, 20)  # Set y-axis limit slightly above the highest accuracy for better visualization\n",
    "plt.title('Accuracy for Different Models(Considered for Hyperparameter Tuning)')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Adding the accuracy values on top of the bars\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 1, f\"{acc:.2f}%\", ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('BertTensor.png', format='png')  # Save as PNG file\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb51d209-7422-4ec8-ae64-d98ea9cef38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a512f3-5aca-45e8-855d-0774df25ff54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87adc56-7191-4677-a6a9-127bcb50d9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7c5b5-30bf-4a14-b894-ebf07f1c8334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a24b2-a716-47ac-8a08-58e9d0ea03da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45d24e-e517-4e98-98cd-ec500fa7ecdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd55832-9719-483e-800a-70d71116e74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
