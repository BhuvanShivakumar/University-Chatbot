{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbeda40-e400-4371-b40f-db89e65520e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "621b3300-1a2b-48e6-b189-593e454f30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, hinge_loss\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "with open('Combined_training.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([char for char in sentence if char not in string.punctuation])\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Extract patterns and tags\n",
    "patterns = []\n",
    "tags = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        patterns.append(preprocess(pattern))\n",
    "        tags.append(intent['tag'])\n",
    "\n",
    "# Convert tags to numerical labels\n",
    "unique_tags = list(set(tags))\n",
    "tag_to_num = {tag: num for num, tag in enumerate(unique_tags)}\n",
    "num_to_tag = {num: tag for tag, num in tag_to_num.items()}\n",
    "y = np.array([tag_to_num[tag] for tag in tags])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "511c5621-e088-490b-b91c-7a4259b4d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize patterns using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(patterns)\n",
    "\n",
    "# Vectorize patterns using Bag-of-Words\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(patterns)\n",
    "\n",
    "# Combine TF-IDF and CountVectorizer features\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_combined = hstack([X_tfidf, X_count])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee0486c2-7080-481d-810b-ef10965c0288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Model Metrics:\n",
      "Training Time: 0.02 seconds\n",
      "Validation Accuracy: 1.0000\n",
      "Train Hinge Loss: 0.0000\n",
      "Validation Hinge Loss: 0.0000\n",
      "Cross-Validation Scores: [1. 1. 1. 1. 1.]\n",
      "Mean Cross-Validation Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training set and measure training time\n",
    "start_time = time.time()\n",
    "svm_model_combined = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear', probability=True))\n",
    "svm_model_combined.fit(X_train, y_train)\n",
    "training_time_combined = time.time() - start_time\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_combined = svm_model_combined.predict(X_val)\n",
    "\n",
    "# Calculate validation accuracy\n",
    "val_accuracy_combined = accuracy_score(y_val, y_val_pred_combined)\n",
    "\n",
    "# Calculate hinge loss on training and validation sets\n",
    "y_train_decision_combined = svm_model_combined.decision_function(X_train)\n",
    "y_val_decision_combined = svm_model_combined.decision_function(X_val)\n",
    "train_hinge_loss_combined = hinge_loss(y_train, y_train_decision_combined, labels=list(tag_to_num.values()))\n",
    "val_hinge_loss_combined = hinge_loss(y_val, y_val_decision_combined, labels=list(tag_to_num.values()))\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_scores_combined = cross_val_score(svm_model_combined, X_combined, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print combined model metrics\n",
    "print(\"\\nCombined Model Metrics:\")\n",
    "print(f'Training Time: {training_time_combined:.2f} seconds')\n",
    "print(f'Validation Accuracy: {val_accuracy_combined:.4f}')\n",
    "print(f'Train Hinge Loss: {train_hinge_loss_combined:.4f}')\n",
    "print(f'Validation Hinge Loss: {val_hinge_loss_combined:.4f}')\n",
    "print(f'Cross-Validation Scores: {cross_val_scores_combined}')\n",
    "print(f'Mean Cross-Validation Score: {np.mean(cross_val_scores_combined):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c98adfdc-8b58-4834-aa2a-acfd2e2cb936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot using Combined Model is ready! Type 'quit' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Good day! What do you need help with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is the use of ai tools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Good day! What do you need help with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "def get_response_combined(user_input):\n",
    "    user_input = preprocess(user_input)\n",
    "    X_user_tfidf = tfidf_vectorizer.transform([user_input])\n",
    "    X_user_count = count_vectorizer.transform([user_input])\n",
    "    X_user_combined = hstack([X_user_tfidf, X_user_count])\n",
    "    tag_prob = svm_model_combined.predict_proba(X_user_combined)[0]\n",
    "    tag_index = np.argmax(tag_prob)\n",
    "    tag = num_to_tag[tag_index]\n",
    "    \n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            return random.choice(intent['responses'])\n",
    "\n",
    "print(\"Chatbot using Combined Model is ready! Type 'quit' to exit.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = get_response_combined(user_input)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20fd71c-09ca-4653-ba4a-b337f5788e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec0d98f-43f5-4c05-8ac6-6ee619c42f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import hinge_loss, accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "with open('Combined_training.json') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "653107e5-9aa9-4771-be7b-7b6ca72531ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([char for char in sentence if char not in string.punctuation])\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Extract patterns and tags\n",
    "patterns = []\n",
    "tags = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        patterns.append(preprocess(pattern))\n",
    "        tags.append(intent['tag'])\n",
    "\n",
    "# Convert tags to numerical labels\n",
    "unique_tags = list(set(tags))\n",
    "tag_to_num = {tag: num for num, tag in enumerate(unique_tags)}\n",
    "num_to_tag = {num: tag for tag, num in tag_to_num.items()}\n",
    "y = np.array([tag_to_num[tag] for tag in tags])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e877f6-2ba1-414f-917b-ad9f76d02c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize patterns using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(patterns)\n",
    "\n",
    "# Vectorize patterns using Bag-of-Words\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(patterns)\n",
    "\n",
    "# Combine TF-IDF and CountVectorizer features\n",
    "X_combined = hstack([X_tfidf, X_count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da255d89-f2bf-4141-8b4e-69d191d9154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d1df9e-18ba-4f64-aa6d-fe9090770182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVM model\n",
    "svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(probability=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c60e8b95-6a37-4ca1-b33a-6d185c66612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (Grid Search): {'svc__C': 0.01, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'linear'}\n",
      "Validation Accuracy (Grid Search): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'svc__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'svc__degree': [2, 3, 4, 5],\n",
    "    'svc__gamma': ['scale', 'auto', 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from Grid Search\n",
    "best_params_grid = grid_search.best_params_\n",
    "print(f\"Best parameters (Grid Search): {best_params_grid}\")\n",
    "\n",
    "# Train and evaluate the Grid Search best model\n",
    "svm_model_grid = make_pipeline(\n",
    "    StandardScaler(with_mean=False), \n",
    "    SVC(C=best_params_grid['svc__C'], degree=best_params_grid['svc__degree'], gamma=best_params_grid['svc__gamma'], kernel=best_params_grid['svc__kernel'], probability=True)\n",
    ")\n",
    "svm_model_grid.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred_grid = svm_model_grid.predict(X_val)\n",
    "val_accuracy_grid = accuracy_score(y_val, y_val_pred_grid)\n",
    "\n",
    "print(f\"Validation Accuracy (Grid Search): {val_accuracy_grid:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdc0d0b0-cf54-455b-978c-6d0d68a8a7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (Random Search): {'svc__kernel': 'linear', 'svc__gamma': 10, 'svc__degree': 4, 'svc__C': 35.93813663804626}\n",
      "Validation Accuracy (Random Search): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'svc__C': np.logspace(-2, 2, 10),  # 10 values between 0.01 and 100\n",
    "    'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'svc__degree': [2, 3, 4, 5],  # Degrees for polynomial kernel\n",
    "    'svc__gamma': ['scale', 'auto', 0.01, 0.1, 1, 10]  # Various gamma values\n",
    "}\n",
    "\n",
    "# Perform Random Search with Cross-Validation\n",
    "random_search = RandomizedSearchCV(svm_model, param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from Random Search\n",
    "best_params_random = random_search.best_params_\n",
    "print(f\"Best parameters (Random Search): {best_params_random}\")\n",
    "\n",
    "# Train and evaluate the Random Search best model\n",
    "svm_model_random = make_pipeline(\n",
    "    StandardScaler(with_mean=False), \n",
    "    SVC(C=best_params_random['svc__C'], degree=best_params_random['svc__degree'], gamma=best_params_random['svc__gamma'], kernel=best_params_random['svc__kernel'], probability=True)\n",
    ")\n",
    "svm_model_random.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred_random = svm_model_random.predict(X_val)\n",
    "val_accuracy_random = accuracy_score(y_val, y_val_pred_random)\n",
    "\n",
    "print(f\"Validation Accuracy (Random Search): {val_accuracy_random:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a54dcdc-c7ab-41fc-a9d8-b5a55ccc762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model selected from Random Search.\n"
     ]
    }
   ],
   "source": [
    "# Select the best model\n",
    "if val_accuracy_grid > val_accuracy_random:\n",
    "    best_model = svm_model_grid\n",
    "    best_params = best_params_grid\n",
    "    print(\"Best model selected from Grid Search.\")\n",
    "else:\n",
    "    best_model = svm_model_random\n",
    "    best_params = best_params_random\n",
    "    print(\"Best model selected from Random Search.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b08cfe-e2b3-40ed-89be-9ce52325f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metrics:\n",
      "Validation Accuracy: 1.0000\n",
      "Train Hinge Loss: 0.0000\n",
      "Validation Hinge Loss: 0.0000\n",
      "Cross-Validation Scores: [1. 1. 1. 1. 1.]\n",
      "Mean Cross-Validation Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Calculate hinge loss on training and validation sets\n",
    "y_train_decision = best_model.decision_function(X_train)\n",
    "y_val_decision = best_model.decision_function(X_val)\n",
    "train_hinge_loss = hinge_loss(y_train, y_train_decision, labels=list(tag_to_num.values()))\n",
    "val_hinge_loss = hinge_loss(y_val, y_val_decision, labels=list(tag_to_num.values()))\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_scores = cross_val_score(best_model, X_combined, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(f'Validation Accuracy: {accuracy_score(y_val, y_val_pred_grid if best_model == svm_model_grid else y_val_pred_random):.4f}')\n",
    "print(f'Train Hinge Loss: {train_hinge_loss:.4f}')\n",
    "print(f'Validation Hinge Loss: {val_hinge_loss:.4f}')\n",
    "print(f'Cross-Validation Scores: {cross_val_scores}')\n",
    "print(f'Mean Cross-Validation Score: {np.mean(cross_val_scores):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313f01d-161f-461c-b4fe-00a02ce7fd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot using Combined Model is ready! Type 'quit' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Good day! What do you need help with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  mai i know more on external examiners \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hey! How can I help?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  may i know more on external examiners\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Good day! What do you need help with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is External Examiners\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The role of the External Examiner In the UK Higher Education system, Universities are responsible for the academic standards of the awards that they offer, and for the quality of the education they provide to enable students to meet those standards. The role of the external examiner is an essential part of the University’s quality assurance processes. They essentially externally ‘audit’ the programmes that they are appointed to, in terms of the attainment of academic standards and the quality of the education. The University of Hertfordshire appoints external examiners mainly from other Universities, but also from industry and/or the professions. They are qualified and experienced in the subject, have an understanding of the academic standards required for the award and are also independent of the University of Hertfordshire. As such, they are able to provide carefully considered advice on the academic standards of the programmes and/or modules to which they have been appointed, and can offer advice on good practice and how to enhance the quality of those programmes/modules. They are also able to offer an informed view of how standards compare with the same or similar awards at other UK Universities of which they have experience. It is important to note that whilst external examiners are full members of the Boards of Examiners to which they are appointed, the decisions of the Board are the collective, consensus views of all members of the Board. Another important feature of external examining in the UK is the provision of annual written reports to the University, based on what they have observed of the institution's assessment processes and the sample of student work that they have seen.  These reports provide invaluable independent feedback to the University at module and/or programme level. The University of Hertfordshire recognises the importance of the role of students in the management of academic standards and quality. External examiners' reports are therefore made available to student representatives, as part of the programme monitoring process. If you are not a student representative and would like to request a copy of the External Examiners’ reports relating to your programme, then please emailaqo@herts.ac.uk [mailto:aqo@herts.ac.uk], stating your ID number, the full title of your programme and your current year/level. A list of external examiners, by subject area, is also available viaaqo@herts.ac.uk [mailto:aqo@herts.ac.uk]. Please note that contacting external examiners regarding any aspect of your programme of study is prohibited. The University has appropriate internal mechanisms in place if you wish to raise a concern using the complaints or appeals procedures, as appropriate (links below): Student complaints procedure [https://ask.herts.ac.uk/making-a-complaint] Student appeals procedure [https://ask.herts.ac.uk/academic-appeals-requests-for-the-review-of-assessment-decisions]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is mental healthj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hi there! What can I help you with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is mental health\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Good day! What do you need help with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Tell me about wellbeing and mental health support\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: If your studies are affecting your mental health and wellbeing then pleasecontact our Student Wellbeing team [https://ask.herts.ac.uk/student-wellbeing]for support. You can alsocall our free 24/7 wellbeing helpline [https://ask.herts.ac.uk/student-wellbeing-24-hour-helpline]ran by Health Assured whenever you need to. You can also talk to staff in your School if you’re struggling.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is Artificial intelligence AI tools?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Information about the University's policy on the use of artificial intelligence tools. Artificial intelligence tools (AI) have had lots of coverage in the news recently including how they can be used in workplaces and universities. You may have heard of tools such as Chat GPT, DALLE-2, Co-Pilot, and Google Bard although there are many more available for different purposes. When it comes to your course, inappropriate use of these tools can negatively impact your learning as well as affecting your own confidence in your qualification and ability. While such tools may seem like time-savers, their potential and limitations are still not fully explored. So far, we know that some materials/information may be out of date or incorrect, and some of the information may be fictitious or contain false references and quotes. We're also aware that since AI models are trained on the data that they are exposed to, this can result in biases. So, responses or information you pull out of such tools may reflect these biases and demonstrate discriminatory attitudes and beliefs.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Explain When could I use an AI tool?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The only occasions where you may use AI tools in your assessment is if you have explicit permission from your tutor in your assessment brief. Your assessment brief will include information on how to declare any use of such tools, and you can speak to your tutor for guidance. If you do not reference your use, then this will constitute academic misconduct. Our current University policy on academic misconduct adequately covers the misuse of such tools, but we are updating them to be clearer on the matter. Unauthorised use of artificially generated material (AI) in researching or presenting material for an assessment is an academic misconduct offence if you use AI tools in producing your assessment unless the use of AI tools is expressly permitted. However, even if expressly permitted, where you do not declare that you have used an artificial intelligence tool(s) in the production of your assessment, or you are dishonest about the extent to which such tools have been used, you will have committed academic misconduct.\n"
     ]
    }
   ],
   "source": [
    "def get_response(user_input):\n",
    "    user_input = preprocess(user_input)\n",
    "    X_user_tfidf = tfidf_vectorizer.transform([user_input])\n",
    "    X_user_count = count_vectorizer.transform([user_input])\n",
    "    X_user_combined = hstack([X_user_tfidf, X_user_count])\n",
    "    tag_prob = best_model.predict_proba(X_user_combined)[0]\n",
    "    tag_index = np.argmax(tag_prob)\n",
    "    tag = num_to_tag[tag_index]\n",
    "    \n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            return random.choice(intent['responses'])\n",
    "\n",
    "print(\"Chatbot using Combined Model is ready! Type 'quit' to exit.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = get_response(user_input)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d254c3-97cc-4879-96c8-69fafc188068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a0334-5d80-4e83-9b3c-91787a09af75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08fbe4-b436-4128-abca-793291a83a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb6a52-40f5-495a-ad05-6553c4fbfaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6a0ad-fde0-4872-8cb1-7d90212ccd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe69ee-4a02-41d2-8d49-8455bd1ed667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
