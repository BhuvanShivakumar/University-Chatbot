{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e9cad-7340-4e12-9f20-d63ebbb57a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331af3f-7b72-400c-b72d-66f2b236e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "with open('Combined_training.json') as file:\n",
    "    data = json.load(file)\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([char for char in sentence if char not in string.punctuation])\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Extract patterns and tags\n",
    "patterns = []\n",
    "tags = []\n",
    "all_responses = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        patterns.append(preprocess(pattern))\n",
    "        tags.append(intent['tag'])\n",
    "    all_responses.extend(intent['responses'])\n",
    "\n",
    "# Encode the tags\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(tags)\n",
    "\n",
    "# Tokenize and pad the patterns\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(patterns)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(patterns)\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# TF-IDF vectorizer for responses\n",
    "response_vectorizer = TfidfVectorizer().fit(all_responses)\n",
    "response_vectors = response_vectorizer.transform(all_responses)\n",
    "\n",
    "# Define the LSTM model creation function\n",
    "def create_model(input_length, vocab_size, num_classes, embedding_dim=128, lstm_units=128, dense_units=64, dropout_rate=0.4, \n",
    "                 learning_rate=0.0005, lstm_dropout=0.3, recurrent_dropout=0.3, optimizer_choice='Adam', activation='relu', regularizer=None):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "    model.add(LSTM(lstm_units, return_sequences=True, dropout=lstm_dropout, recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units, dropout=lstm_dropout, recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(dense_units, activation=activation, kernel_regularizer=regularizer))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    if optimizer_choice == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'SGD':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'Adamax':\n",
    "        optimizer = Adamax(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "training_times = []\n",
    "val_accuracies = []\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "for train_index, val_index in kf.split(padded_sequences):\n",
    "    print(f\"Fold {fold}\")\n",
    "    X_train, X_val = padded_sequences[train_index], padded_sequences[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    model = create_model(input_length=padded_sequences.shape[1], vocab_size=len(word_index)+1, num_classes=len(label_encoder.classes_),\n",
    "                         embedding_dim=128, lstm_units=128, dense_units=64, dropout_rate=0.4, learning_rate=0.0005, \n",
    "                         lstm_dropout=0.3, recurrent_dropout=0.3, optimizer_choice='Adam', activation='relu', regularizer=l2(0.01))\n",
    "    \n",
    "    # Train the model and measure training time\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n",
    "    training_time = time.time() - start_time\n",
    "    training_times.append(training_time)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred_classes)\n",
    "    val_precision = precision_score(y_val, y_val_pred_classes, average='weighted')\n",
    "    val_recall = recall_score(y_val, y_val_pred_classes, average='weighted')\n",
    "    val_f1 = f1_score(y_val, y_val_pred_classes, average='weighted')\n",
    "    \n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f'Training Time: {training_time:.2f} seconds')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'Precision: {val_precision:.4f}')\n",
    "    print(f'Recall: {val_recall:.4f}')\n",
    "    print(f'F1 Score: {val_f1:.4f}')\n",
    "    print(\"-\" * 30)\n",
    "    fold += 1\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics Over All Folds:\")\n",
    "print(f'Average Training Time: {np.mean(training_times):.2f} seconds')\n",
    "print(f'Average Validation Accuracy: {np.mean(val_accuracies):.4f}')\n",
    "print(f'Average Precision: {np.mean(val_precisions):.4f}')\n",
    "print(f'Average Recall: {np.mean(val_recalls):.4f}')\n",
    "print(f'Average F1 Score: {np.mean(val_f1s):.4f}')\n",
    "\n",
    "def get_response(user_input):\n",
    "    user_input = preprocess(user_input)\n",
    "    seq = tokenizer.texts_to_sequences([user_input])\n",
    "    padded_seq = pad_sequences(seq, maxlen=padded_sequences.shape[1], padding='post')\n",
    "    pred = model.predict(padded_seq)\n",
    "    tag_index = np.argmax(pred)\n",
    "    tag = label_encoder.inverse_transform([tag_index])[0]\n",
    "\n",
    "    if max(pred[0]) < 0.6:  # Adjust threshold for confidence\n",
    "        user_vector = response_vectorizer.transform([user_input])\n",
    "        similarity_scores = cosine_similarity(user_vector, response_vectors)\n",
    "        best_match_index = np.argmax(similarity_scores)\n",
    "        best_match_response = all_responses[best_match_index]\n",
    "        \n",
    "        for intent in data['intents']:\n",
    "            if best_match_response in intent['responses']:\n",
    "                tag = intent['tag']\n",
    "                break\n",
    "\n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            return random.choice(intent['responses'])\n",
    "\n",
    "print(\"Chatbot is ready! Type 'quit' to exit.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = get_response(user_input)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8da816-198c-4a87-a308-13ae25bfd4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7800930a-87c3-46bc-9808-1f069613747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open('Combined_training.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([char for char in sentence if char not in string.punctuation])\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Extract patterns and tags\n",
    "patterns = []\n",
    "tags = []\n",
    "all_responses = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        patterns.append(preprocess(pattern))\n",
    "        tags.append(intent['tag'])\n",
    "    all_responses.extend(intent['responses'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5435c86-0eeb-44d7-a717-a4ed9b2c8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the tags\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(tags)\n",
    "\n",
    "# Tokenize and pad the patterns\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(patterns)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(patterns)\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# TF-IDF vectorizer for responses\n",
    "response_vectorizer = TfidfVectorizer().fit(all_responses)\n",
    "response_vectors = response_vectorizer.transform(all_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701a6ed0-8fe7-4ded-a6b9-f484a20bbfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model creation function\n",
    "def create_model(input_length, vocab_size, num_classes, embedding_dim=128, lstm_units=128, dense_units=64, dropout_rate=0.4, \n",
    "                 learning_rate=0.0005, lstm_dropout=0.3, recurrent_dropout=0.3, optimizer_choice='Adam', activation='relu', regularizer=None):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "    model.add(LSTM(lstm_units, return_sequences=True, dropout=lstm_dropout, recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units, dropout=lstm_dropout, recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(dense_units, activation=activation, kernel_regularizer=regularizer))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    if optimizer_choice == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'SGD':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'Adamax':\n",
    "        optimizer = Adamax(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6aced54-5cc0-4e17-8404-7b738000a2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 161ms/step - loss: 3.6189 - accuracy: 0.0595 - val_loss: 3.5975 - val_accuracy: 0.1905\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3.5972 - accuracy: 0.1310 - val_loss: 3.5745 - val_accuracy: 0.1905\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.5752 - accuracy: 0.1310 - val_loss: 3.5504 - val_accuracy: 0.1905\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.5563 - accuracy: 0.1310 - val_loss: 3.5258 - val_accuracy: 0.1905\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.5344 - accuracy: 0.1310 - val_loss: 3.5003 - val_accuracy: 0.1905\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.5146 - accuracy: 0.1310 - val_loss: 3.4726 - val_accuracy: 0.1905\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.4815 - accuracy: 0.1310 - val_loss: 3.4414 - val_accuracy: 0.1905\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.4576 - accuracy: 0.1310 - val_loss: 3.4060 - val_accuracy: 0.1905\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.4167 - accuracy: 0.1310 - val_loss: 3.3607 - val_accuracy: 0.1905\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.3975 - accuracy: 0.1310 - val_loss: 3.3043 - val_accuracy: 0.1905\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.3511 - accuracy: 0.1310 - val_loss: 3.2427 - val_accuracy: 0.1905\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.3468 - accuracy: 0.1429 - val_loss: 3.1851 - val_accuracy: 0.2381\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.2807 - accuracy: 0.1429 - val_loss: 3.1351 - val_accuracy: 0.2381\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.2704 - accuracy: 0.1667 - val_loss: 3.0851 - val_accuracy: 0.2381\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.1984 - accuracy: 0.2381 - val_loss: 3.0384 - val_accuracy: 0.2381\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.1439 - accuracy: 0.2143 - val_loss: 3.0007 - val_accuracy: 0.2381\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.1174 - accuracy: 0.2619 - val_loss: 2.9633 - val_accuracy: 0.2381\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.0818 - accuracy: 0.2143 - val_loss: 2.9134 - val_accuracy: 0.2381\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.0287 - accuracy: 0.2619 - val_loss: 2.8684 - val_accuracy: 0.2381\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.9899 - accuracy: 0.2381 - val_loss: 2.8413 - val_accuracy: 0.2381\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.9615 - accuracy: 0.2262 - val_loss: 2.8274 - val_accuracy: 0.2381\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.9174 - accuracy: 0.2024 - val_loss: 2.8172 - val_accuracy: 0.2381\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.8995 - accuracy: 0.2500 - val_loss: 2.8064 - val_accuracy: 0.2381\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.8894 - accuracy: 0.2500 - val_loss: 2.7888 - val_accuracy: 0.2381\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.8547 - accuracy: 0.2738 - val_loss: 2.7653 - val_accuracy: 0.2381\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.8079 - accuracy: 0.2619 - val_loss: 2.7380 - val_accuracy: 0.2381\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.7882 - accuracy: 0.2143 - val_loss: 2.7077 - val_accuracy: 0.2381\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.7111 - accuracy: 0.2857 - val_loss: 2.6730 - val_accuracy: 0.2381\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.6857 - accuracy: 0.2738 - val_loss: 2.6354 - val_accuracy: 0.2381\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.6096 - accuracy: 0.3095 - val_loss: 2.5968 - val_accuracy: 0.2381\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.5817 - accuracy: 0.3214 - val_loss: 2.5485 - val_accuracy: 0.2381\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.5457 - accuracy: 0.3452 - val_loss: 2.4807 - val_accuracy: 0.2381\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.4744 - accuracy: 0.3214 - val_loss: 2.4195 - val_accuracy: 0.2381\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.4111 - accuracy: 0.3810 - val_loss: 2.3853 - val_accuracy: 0.2857\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.3999 - accuracy: 0.3690 - val_loss: 2.3414 - val_accuracy: 0.2857\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.3422 - accuracy: 0.3929 - val_loss: 2.3119 - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.2967 - accuracy: 0.4167 - val_loss: 2.2971 - val_accuracy: 0.3810\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.2101 - accuracy: 0.4524 - val_loss: 2.2636 - val_accuracy: 0.3333\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.2070 - accuracy: 0.4762 - val_loss: 2.2376 - val_accuracy: 0.3333\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.1325 - accuracy: 0.5476 - val_loss: 2.2244 - val_accuracy: 0.3333\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.1054 - accuracy: 0.4762 - val_loss: 2.1874 - val_accuracy: 0.3810\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.0761 - accuracy: 0.5119 - val_loss: 2.1488 - val_accuracy: 0.3333\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.0365 - accuracy: 0.5119 - val_loss: 2.1153 - val_accuracy: 0.3333\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.9897 - accuracy: 0.5357 - val_loss: 2.0764 - val_accuracy: 0.3333\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.9669 - accuracy: 0.5476 - val_loss: 2.0432 - val_accuracy: 0.3810\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.9016 - accuracy: 0.5476 - val_loss: 1.9879 - val_accuracy: 0.3810\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.8212 - accuracy: 0.5952 - val_loss: 1.9367 - val_accuracy: 0.3810\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.7882 - accuracy: 0.5833 - val_loss: 1.8795 - val_accuracy: 0.3810\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.8208 - accuracy: 0.5476 - val_loss: 1.8394 - val_accuracy: 0.4762\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.7212 - accuracy: 0.5952 - val_loss: 1.8073 - val_accuracy: 0.4286\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.7055 - accuracy: 0.5357 - val_loss: 1.7573 - val_accuracy: 0.4286\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.6381 - accuracy: 0.7024 - val_loss: 1.6925 - val_accuracy: 0.6667\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.6261 - accuracy: 0.6429 - val_loss: 1.6503 - val_accuracy: 0.5714\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.5490 - accuracy: 0.6786 - val_loss: 1.6069 - val_accuracy: 0.7143\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.5256 - accuracy: 0.7143 - val_loss: 1.5455 - val_accuracy: 0.6667\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.4860 - accuracy: 0.7381 - val_loss: 1.4812 - val_accuracy: 0.8095\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.4608 - accuracy: 0.7500 - val_loss: 1.4377 - val_accuracy: 0.9048\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.4373 - accuracy: 0.7262 - val_loss: 1.4025 - val_accuracy: 0.9048\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.4077 - accuracy: 0.7500 - val_loss: 1.3685 - val_accuracy: 0.9048\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.4079 - accuracy: 0.7262 - val_loss: 1.3423 - val_accuracy: 0.9048\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.3643 - accuracy: 0.7262 - val_loss: 1.3040 - val_accuracy: 0.9524\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.2917 - accuracy: 0.8214 - val_loss: 1.2661 - val_accuracy: 0.9524\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.2819 - accuracy: 0.8452 - val_loss: 1.2645 - val_accuracy: 0.9524\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.2829 - accuracy: 0.7857 - val_loss: 1.2114 - val_accuracy: 0.9524\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.2142 - accuracy: 0.8333 - val_loss: 1.1873 - val_accuracy: 0.9524\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.2197 - accuracy: 0.8095 - val_loss: 1.1875 - val_accuracy: 0.9048\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1943 - accuracy: 0.7976 - val_loss: 1.1757 - val_accuracy: 0.9048\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1721 - accuracy: 0.8095 - val_loss: 1.1269 - val_accuracy: 0.9048\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1461 - accuracy: 0.8452 - val_loss: 1.0820 - val_accuracy: 0.9048\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.1565 - accuracy: 0.8690 - val_loss: 1.0700 - val_accuracy: 0.9048\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1910 - accuracy: 0.7976 - val_loss: 1.0246 - val_accuracy: 0.9048\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0914 - accuracy: 0.8810 - val_loss: 0.9793 - val_accuracy: 0.9524\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.0806 - accuracy: 0.8571 - val_loss: 0.9639 - val_accuracy: 0.9524\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.0578 - accuracy: 0.9167 - val_loss: 0.9665 - val_accuracy: 0.9524\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.0667 - accuracy: 0.8690 - val_loss: 0.9664 - val_accuracy: 0.9524\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.0278 - accuracy: 0.8810 - val_loss: 0.9551 - val_accuracy: 0.9048\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.9953 - accuracy: 0.8929 - val_loss: 0.9414 - val_accuracy: 0.9048\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.9626 - accuracy: 0.9405 - val_loss: 0.9214 - val_accuracy: 0.9524\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.9561 - accuracy: 0.9048 - val_loss: 0.9142 - val_accuracy: 0.9048\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.9906 - accuracy: 0.8690 - val_loss: 0.8540 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.9430 - accuracy: 0.9167 - val_loss: 0.8470 - val_accuracy: 0.9524\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.9601 - accuracy: 0.9167 - val_loss: 0.8332 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.9373 - accuracy: 0.8571 - val_loss: 0.8255 - val_accuracy: 0.9524\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8918 - accuracy: 0.9048 - val_loss: 0.8417 - val_accuracy: 0.9524\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.9237 - accuracy: 0.8690 - val_loss: 0.8665 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8641 - accuracy: 0.9167 - val_loss: 0.8911 - val_accuracy: 0.9048\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.9141 - accuracy: 0.8929 - val_loss: 0.8740 - val_accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.8829 - accuracy: 0.9524 - val_loss: 0.8052 - val_accuracy: 0.9524\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.8608 - accuracy: 0.8929 - val_loss: 0.7603 - val_accuracy: 0.9524\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7994 - accuracy: 0.9405 - val_loss: 0.7491 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8005 - accuracy: 0.9524 - val_loss: 0.7316 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7910 - accuracy: 0.9167 - val_loss: 0.7021 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7635 - accuracy: 0.9643 - val_loss: 0.6938 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8023 - accuracy: 0.9405 - val_loss: 0.6864 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7554 - accuracy: 0.9524 - val_loss: 0.6928 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7791 - accuracy: 0.9405 - val_loss: 0.7152 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7259 - accuracy: 0.9405 - val_loss: 0.7404 - val_accuracy: 0.9048\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7119 - accuracy: 0.9881 - val_loss: 0.7078 - val_accuracy: 0.9524\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7360 - accuracy: 0.9643 - val_loss: 0.6638 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7368 - accuracy: 0.9405 - val_loss: 0.6355 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "Training Time: 9.91 seconds\n",
      "Validation Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "------------------------------\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 149ms/step - loss: 3.6287 - accuracy: 0.0595 - val_loss: 3.6154 - val_accuracy: 0.1429\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 3.6050 - accuracy: 0.1786 - val_loss: 3.5967 - val_accuracy: 0.1429\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3.5829 - accuracy: 0.1548 - val_loss: 3.5783 - val_accuracy: 0.1429\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.5616 - accuracy: 0.1429 - val_loss: 3.5588 - val_accuracy: 0.1429\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.5268 - accuracy: 0.1429 - val_loss: 3.5365 - val_accuracy: 0.1429\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.5047 - accuracy: 0.1429 - val_loss: 3.5116 - val_accuracy: 0.1429\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.4745 - accuracy: 0.1429 - val_loss: 3.4857 - val_accuracy: 0.1429\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.4421 - accuracy: 0.1429 - val_loss: 3.4571 - val_accuracy: 0.1429\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.4041 - accuracy: 0.1429 - val_loss: 3.4282 - val_accuracy: 0.1429\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.3456 - accuracy: 0.1429 - val_loss: 3.4103 - val_accuracy: 0.1429\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.3133 - accuracy: 0.1429 - val_loss: 3.4107 - val_accuracy: 0.1429\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.2735 - accuracy: 0.1548 - val_loss: 3.4038 - val_accuracy: 0.1429\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.2759 - accuracy: 0.2024 - val_loss: 3.3601 - val_accuracy: 0.1429\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.2290 - accuracy: 0.2262 - val_loss: 3.3006 - val_accuracy: 0.1429\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.1345 - accuracy: 0.2262 - val_loss: 3.2489 - val_accuracy: 0.1429\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.1026 - accuracy: 0.2262 - val_loss: 3.2037 - val_accuracy: 0.1429\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.0635 - accuracy: 0.2381 - val_loss: 3.1537 - val_accuracy: 0.1429\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.9960 - accuracy: 0.2143 - val_loss: 3.1029 - val_accuracy: 0.1429\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.9903 - accuracy: 0.2619 - val_loss: 3.0629 - val_accuracy: 0.1429\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2.9271 - accuracy: 0.2262 - val_loss: 3.0407 - val_accuracy: 0.1429\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.9317 - accuracy: 0.2143 - val_loss: 3.0349 - val_accuracy: 0.1429\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.8776 - accuracy: 0.2500 - val_loss: 3.0411 - val_accuracy: 0.1429\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.8364 - accuracy: 0.2381 - val_loss: 3.0484 - val_accuracy: 0.1429\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.8067 - accuracy: 0.2619 - val_loss: 3.0461 - val_accuracy: 0.1429\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.8048 - accuracy: 0.2381 - val_loss: 3.0283 - val_accuracy: 0.1429\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.7476 - accuracy: 0.2500 - val_loss: 2.9994 - val_accuracy: 0.1429\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.7070 - accuracy: 0.2619 - val_loss: 2.9752 - val_accuracy: 0.1429\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.6977 - accuracy: 0.2500 - val_loss: 2.9535 - val_accuracy: 0.1429\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.6440 - accuracy: 0.2976 - val_loss: 2.9319 - val_accuracy: 0.1905\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.5860 - accuracy: 0.2738 - val_loss: 2.9121 - val_accuracy: 0.1905\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.5763 - accuracy: 0.3214 - val_loss: 2.8886 - val_accuracy: 0.1905\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.4880 - accuracy: 0.2619 - val_loss: 2.8529 - val_accuracy: 0.1905\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.4906 - accuracy: 0.2976 - val_loss: 2.8231 - val_accuracy: 0.1905\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.4287 - accuracy: 0.3214 - val_loss: 2.7941 - val_accuracy: 0.1905\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.4087 - accuracy: 0.3810 - val_loss: 2.7567 - val_accuracy: 0.1905\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.3446 - accuracy: 0.3810 - val_loss: 2.7164 - val_accuracy: 0.1905\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.3093 - accuracy: 0.3690 - val_loss: 2.6679 - val_accuracy: 0.2381\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.2514 - accuracy: 0.4048 - val_loss: 2.6322 - val_accuracy: 0.2381\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.2322 - accuracy: 0.4048 - val_loss: 2.5960 - val_accuracy: 0.2381\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.1818 - accuracy: 0.4524 - val_loss: 2.5544 - val_accuracy: 0.2381\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.1252 - accuracy: 0.4286 - val_loss: 2.5086 - val_accuracy: 0.2381\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.0804 - accuracy: 0.5238 - val_loss: 2.4629 - val_accuracy: 0.2857\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.0195 - accuracy: 0.5357 - val_loss: 2.4069 - val_accuracy: 0.3810\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.9833 - accuracy: 0.5000 - val_loss: 2.3591 - val_accuracy: 0.4286\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.9520 - accuracy: 0.5119 - val_loss: 2.3219 - val_accuracy: 0.3810\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.9138 - accuracy: 0.5833 - val_loss: 2.2950 - val_accuracy: 0.3810\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.8611 - accuracy: 0.5119 - val_loss: 2.2596 - val_accuracy: 0.4286\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.7656 - accuracy: 0.6310 - val_loss: 2.2604 - val_accuracy: 0.3333\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.7782 - accuracy: 0.5833 - val_loss: 2.2172 - val_accuracy: 0.3810\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.7221 - accuracy: 0.6429 - val_loss: 2.1719 - val_accuracy: 0.4286\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.6836 - accuracy: 0.6548 - val_loss: 2.1393 - val_accuracy: 0.4286\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.6407 - accuracy: 0.6429 - val_loss: 2.1047 - val_accuracy: 0.4762\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.6249 - accuracy: 0.6667 - val_loss: 2.0885 - val_accuracy: 0.4286\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.5829 - accuracy: 0.6071 - val_loss: 2.0354 - val_accuracy: 0.4762\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.5598 - accuracy: 0.6667 - val_loss: 1.9248 - val_accuracy: 0.4762\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.4663 - accuracy: 0.7143 - val_loss: 1.8236 - val_accuracy: 0.4762\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.4827 - accuracy: 0.6905 - val_loss: 1.7412 - val_accuracy: 0.4762\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.3898 - accuracy: 0.7500 - val_loss: 1.7100 - val_accuracy: 0.4762\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.3704 - accuracy: 0.7381 - val_loss: 1.7014 - val_accuracy: 0.4286\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.4352 - accuracy: 0.6786 - val_loss: 1.5996 - val_accuracy: 0.5238\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.3183 - accuracy: 0.7738 - val_loss: 1.5425 - val_accuracy: 0.5238\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.3172 - accuracy: 0.8095 - val_loss: 1.5371 - val_accuracy: 0.5714\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.2854 - accuracy: 0.7976 - val_loss: 1.4681 - val_accuracy: 0.6190\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.2522 - accuracy: 0.7738 - val_loss: 1.4452 - val_accuracy: 0.6190\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.2542 - accuracy: 0.8214 - val_loss: 1.4554 - val_accuracy: 0.5714\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.2029 - accuracy: 0.7976 - val_loss: 1.3825 - val_accuracy: 0.5238\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1806 - accuracy: 0.7976 - val_loss: 1.3030 - val_accuracy: 0.6190\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.1863 - accuracy: 0.7619 - val_loss: 1.2556 - val_accuracy: 0.7143\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1401 - accuracy: 0.7976 - val_loss: 1.2209 - val_accuracy: 0.8095\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1355 - accuracy: 0.7857 - val_loss: 1.2299 - val_accuracy: 0.8095\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0818 - accuracy: 0.8810 - val_loss: 1.2669 - val_accuracy: 0.7143\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0809 - accuracy: 0.8571 - val_loss: 1.2605 - val_accuracy: 0.6667\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0927 - accuracy: 0.8095 - val_loss: 1.2036 - val_accuracy: 0.7143\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.0081 - accuracy: 0.8571 - val_loss: 1.1098 - val_accuracy: 0.7619\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0260 - accuracy: 0.8333 - val_loss: 1.0670 - val_accuracy: 0.7619\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 1.0366 - accuracy: 0.8214 - val_loss: 1.0541 - val_accuracy: 0.7619\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.0638 - accuracy: 0.8214 - val_loss: 1.0443 - val_accuracy: 0.7619\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.9655 - accuracy: 0.8929 - val_loss: 1.0360 - val_accuracy: 0.8095\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.9121 - accuracy: 0.9167 - val_loss: 1.0624 - val_accuracy: 0.7619\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.9564 - accuracy: 0.8929 - val_loss: 1.0469 - val_accuracy: 0.8095\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.9572 - accuracy: 0.8690 - val_loss: 0.9949 - val_accuracy: 0.8095\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8678 - accuracy: 0.9167 - val_loss: 0.9245 - val_accuracy: 0.9524\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.8794 - accuracy: 0.9167 - val_loss: 0.9034 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8352 - accuracy: 0.9524 - val_loss: 0.9081 - val_accuracy: 0.9524\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8483 - accuracy: 0.9405 - val_loss: 0.9087 - val_accuracy: 0.9524\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8563 - accuracy: 0.9048 - val_loss: 0.8493 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7931 - accuracy: 0.9524 - val_loss: 0.8165 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7861 - accuracy: 0.9762 - val_loss: 0.7997 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8244 - accuracy: 0.9524 - val_loss: 0.7860 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7707 - accuracy: 0.9881 - val_loss: 0.7779 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7734 - accuracy: 0.9524 - val_loss: 0.7846 - val_accuracy: 0.9524\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7766 - accuracy: 0.9286 - val_loss: 0.7675 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7304 - accuracy: 0.9881 - val_loss: 0.7522 - val_accuracy: 0.9524\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7510 - accuracy: 0.9524 - val_loss: 0.8000 - val_accuracy: 0.9048\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7316 - accuracy: 0.9643 - val_loss: 0.8127 - val_accuracy: 0.9048\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7518 - accuracy: 0.9762 - val_loss: 0.7535 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7213 - accuracy: 0.9524 - val_loss: 0.7583 - val_accuracy: 0.9048\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7184 - accuracy: 0.9762 - val_loss: 0.7090 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6604 - accuracy: 0.9881 - val_loss: 0.6734 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6829 - accuracy: 0.9524 - val_loss: 0.6585 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "Training Time: 10.04 seconds\n",
      "Validation Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "------------------------------\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 162ms/step - loss: 3.6400 - accuracy: 0.0238 - val_loss: 3.6185 - val_accuracy: 0.2381\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 3.6166 - accuracy: 0.1310 - val_loss: 3.5965 - val_accuracy: 0.2381\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3.5955 - accuracy: 0.1310 - val_loss: 3.5761 - val_accuracy: 0.2381\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.5724 - accuracy: 0.1310 - val_loss: 3.5526 - val_accuracy: 0.2381\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.5610 - accuracy: 0.1190 - val_loss: 3.5269 - val_accuracy: 0.2381\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.5304 - accuracy: 0.1190 - val_loss: 3.5006 - val_accuracy: 0.2381\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.5157 - accuracy: 0.1190 - val_loss: 3.4715 - val_accuracy: 0.2381\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.4790 - accuracy: 0.1190 - val_loss: 3.4392 - val_accuracy: 0.2381\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.4554 - accuracy: 0.1190 - val_loss: 3.3973 - val_accuracy: 0.2381\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.4181 - accuracy: 0.1190 - val_loss: 3.3469 - val_accuracy: 0.2381\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.3832 - accuracy: 0.1310 - val_loss: 3.2933 - val_accuracy: 0.2381\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.3533 - accuracy: 0.1310 - val_loss: 3.2401 - val_accuracy: 0.2381\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.3204 - accuracy: 0.1667 - val_loss: 3.2066 - val_accuracy: 0.2381\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.2748 - accuracy: 0.1905 - val_loss: 3.1795 - val_accuracy: 0.2381\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.2264 - accuracy: 0.1905 - val_loss: 3.1443 - val_accuracy: 0.2381\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.1803 - accuracy: 0.2143 - val_loss: 3.0971 - val_accuracy: 0.2381\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.1289 - accuracy: 0.2381 - val_loss: 3.0432 - val_accuracy: 0.2381\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.0631 - accuracy: 0.2143 - val_loss: 2.9889 - val_accuracy: 0.2381\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.0366 - accuracy: 0.2024 - val_loss: 2.9383 - val_accuracy: 0.2381\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.9999 - accuracy: 0.1905 - val_loss: 2.9015 - val_accuracy: 0.2381\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.9733 - accuracy: 0.2143 - val_loss: 2.8857 - val_accuracy: 0.2381\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.9781 - accuracy: 0.2024 - val_loss: 2.8880 - val_accuracy: 0.2381\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.9078 - accuracy: 0.2024 - val_loss: 2.9070 - val_accuracy: 0.2381\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.9031 - accuracy: 0.2024 - val_loss: 2.9265 - val_accuracy: 0.2381\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.8157 - accuracy: 0.2024 - val_loss: 2.9130 - val_accuracy: 0.2381\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.8151 - accuracy: 0.2143 - val_loss: 2.8778 - val_accuracy: 0.2381\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.8142 - accuracy: 0.2262 - val_loss: 2.8403 - val_accuracy: 0.2381\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.7404 - accuracy: 0.2738 - val_loss: 2.8148 - val_accuracy: 0.2381\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.6900 - accuracy: 0.3095 - val_loss: 2.7936 - val_accuracy: 0.2381\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.6489 - accuracy: 0.3333 - val_loss: 2.7631 - val_accuracy: 0.2381\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.6216 - accuracy: 0.3095 - val_loss: 2.7166 - val_accuracy: 0.2381\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.5670 - accuracy: 0.2857 - val_loss: 2.6728 - val_accuracy: 0.2381\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.5110 - accuracy: 0.2857 - val_loss: 2.6336 - val_accuracy: 0.2381\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.4243 - accuracy: 0.3690 - val_loss: 2.5992 - val_accuracy: 0.2381\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.3922 - accuracy: 0.2738 - val_loss: 2.5660 - val_accuracy: 0.2381\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.3542 - accuracy: 0.3929 - val_loss: 2.5365 - val_accuracy: 0.2381\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.2666 - accuracy: 0.3810 - val_loss: 2.4970 - val_accuracy: 0.2381\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.2315 - accuracy: 0.5119 - val_loss: 2.4483 - val_accuracy: 0.2857\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.1874 - accuracy: 0.4524 - val_loss: 2.4020 - val_accuracy: 0.2857\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.1118 - accuracy: 0.4762 - val_loss: 2.3582 - val_accuracy: 0.2857\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.0096 - accuracy: 0.5952 - val_loss: 2.3029 - val_accuracy: 0.3333\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.9795 - accuracy: 0.5714 - val_loss: 2.2525 - val_accuracy: 0.3810\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.9167 - accuracy: 0.6310 - val_loss: 2.2045 - val_accuracy: 0.3333\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.8963 - accuracy: 0.6310 - val_loss: 2.1243 - val_accuracy: 0.3810\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.7851 - accuracy: 0.6667 - val_loss: 2.0594 - val_accuracy: 0.3333\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.7745 - accuracy: 0.6071 - val_loss: 1.9963 - val_accuracy: 0.3810\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.7088 - accuracy: 0.6667 - val_loss: 1.9274 - val_accuracy: 0.4286\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.6799 - accuracy: 0.6905 - val_loss: 1.8971 - val_accuracy: 0.3810\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.5935 - accuracy: 0.6667 - val_loss: 1.8348 - val_accuracy: 0.3810\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.5909 - accuracy: 0.6429 - val_loss: 1.7701 - val_accuracy: 0.3810\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.5604 - accuracy: 0.6190 - val_loss: 1.7191 - val_accuracy: 0.5238\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.4905 - accuracy: 0.7143 - val_loss: 1.6818 - val_accuracy: 0.5238\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.4546 - accuracy: 0.7143 - val_loss: 1.6520 - val_accuracy: 0.5238\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.4042 - accuracy: 0.7381 - val_loss: 1.6076 - val_accuracy: 0.5714\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.3637 - accuracy: 0.7381 - val_loss: 1.5755 - val_accuracy: 0.5714\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.3925 - accuracy: 0.7500 - val_loss: 1.5198 - val_accuracy: 0.5714\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.3667 - accuracy: 0.7262 - val_loss: 1.4832 - val_accuracy: 0.5714\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.3319 - accuracy: 0.7500 - val_loss: 1.4555 - val_accuracy: 0.5238\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.3049 - accuracy: 0.7738 - val_loss: 1.4118 - val_accuracy: 0.5714\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.2680 - accuracy: 0.7976 - val_loss: 1.4132 - val_accuracy: 0.5714\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.2729 - accuracy: 0.7738 - val_loss: 1.4514 - val_accuracy: 0.5714\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.2607 - accuracy: 0.7262 - val_loss: 1.3668 - val_accuracy: 0.6190\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.1934 - accuracy: 0.8214 - val_loss: 1.3274 - val_accuracy: 0.6667\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1626 - accuracy: 0.8095 - val_loss: 1.3013 - val_accuracy: 0.6667\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.1415 - accuracy: 0.8333 - val_loss: 1.2819 - val_accuracy: 0.6667\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1331 - accuracy: 0.8452 - val_loss: 1.2806 - val_accuracy: 0.6667\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1600 - accuracy: 0.8214 - val_loss: 1.2537 - val_accuracy: 0.6667\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.0810 - accuracy: 0.8095 - val_loss: 1.2134 - val_accuracy: 0.7143\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1023 - accuracy: 0.8095 - val_loss: 1.1824 - val_accuracy: 0.7143\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0516 - accuracy: 0.8690 - val_loss: 1.1503 - val_accuracy: 0.7619\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0694 - accuracy: 0.8690 - val_loss: 1.1315 - val_accuracy: 0.7619\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0537 - accuracy: 0.8452 - val_loss: 1.1348 - val_accuracy: 0.7619\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0365 - accuracy: 0.8690 - val_loss: 1.1364 - val_accuracy: 0.7143\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0204 - accuracy: 0.8571 - val_loss: 1.1840 - val_accuracy: 0.6667\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.9740 - accuracy: 0.8810 - val_loss: 1.2054 - val_accuracy: 0.6667\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.9770 - accuracy: 0.8452 - val_loss: 1.1474 - val_accuracy: 0.7143\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.9441 - accuracy: 0.9048 - val_loss: 1.0818 - val_accuracy: 0.7143\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.9058 - accuracy: 0.9405 - val_loss: 1.0345 - val_accuracy: 0.7619\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.9816 - accuracy: 0.8452 - val_loss: 1.0153 - val_accuracy: 0.7619\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.9401 - accuracy: 0.8929 - val_loss: 1.0003 - val_accuracy: 0.7619\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.8935 - accuracy: 0.8810 - val_loss: 0.9717 - val_accuracy: 0.8095\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.9335 - accuracy: 0.8929 - val_loss: 0.9413 - val_accuracy: 0.9524\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8690 - accuracy: 0.9048 - val_loss: 0.9189 - val_accuracy: 0.9048\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8517 - accuracy: 0.9048 - val_loss: 0.9087 - val_accuracy: 0.9048\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.8518 - accuracy: 0.9048 - val_loss: 0.8925 - val_accuracy: 0.9524\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.8403 - accuracy: 0.9167 - val_loss: 0.8633 - val_accuracy: 0.9048\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8632 - accuracy: 0.8810 - val_loss: 0.8481 - val_accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.8151 - accuracy: 0.9167 - val_loss: 0.8738 - val_accuracy: 0.8571\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7872 - accuracy: 0.9286 - val_loss: 0.8594 - val_accuracy: 0.8571\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7759 - accuracy: 0.9524 - val_loss: 0.8288 - val_accuracy: 0.9048\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7676 - accuracy: 0.9286 - val_loss: 0.8113 - val_accuracy: 0.9524\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8536 - accuracy: 0.8810 - val_loss: 0.8711 - val_accuracy: 0.9524\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7575 - accuracy: 0.9524 - val_loss: 1.0127 - val_accuracy: 0.9048\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7535 - accuracy: 0.9167 - val_loss: 0.9352 - val_accuracy: 0.9048\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7536 - accuracy: 0.9286 - val_loss: 0.7798 - val_accuracy: 0.9524\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7224 - accuracy: 0.9405 - val_loss: 0.7217 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7374 - accuracy: 0.9286 - val_loss: 0.6856 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.7026 - accuracy: 0.9405 - val_loss: 0.6691 - val_accuracy: 0.9524\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7203 - accuracy: 0.9405 - val_loss: 0.6587 - val_accuracy: 0.9524\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7253 - accuracy: 0.9286 - val_loss: 0.6511 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "Training Time: 9.95 seconds\n",
      "Validation Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "------------------------------\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 149ms/step - loss: 3.6213 - accuracy: 0.1190 - val_loss: 3.6177 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3.5982 - accuracy: 0.1786 - val_loss: 3.6065 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3.5626 - accuracy: 0.1786 - val_loss: 3.5968 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.5319 - accuracy: 0.1786 - val_loss: 3.5891 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.4947 - accuracy: 0.1786 - val_loss: 3.5845 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.4601 - accuracy: 0.1786 - val_loss: 3.5843 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.4081 - accuracy: 0.1786 - val_loss: 3.5920 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.3591 - accuracy: 0.1786 - val_loss: 3.6137 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.2914 - accuracy: 0.1786 - val_loss: 3.6543 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.2693 - accuracy: 0.1786 - val_loss: 3.6847 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.2203 - accuracy: 0.1905 - val_loss: 3.6701 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.1869 - accuracy: 0.2143 - val_loss: 3.6214 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.1406 - accuracy: 0.2262 - val_loss: 3.5624 - val_accuracy: 0.0952\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.0917 - accuracy: 0.2500 - val_loss: 3.5122 - val_accuracy: 0.1429\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.0739 - accuracy: 0.2619 - val_loss: 3.4693 - val_accuracy: 0.1429\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.0037 - accuracy: 0.2738 - val_loss: 3.4277 - val_accuracy: 0.0952\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.9566 - accuracy: 0.2500 - val_loss: 3.3878 - val_accuracy: 0.0952\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.9057 - accuracy: 0.2619 - val_loss: 3.3554 - val_accuracy: 0.0952\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.8742 - accuracy: 0.2381 - val_loss: 3.3355 - val_accuracy: 0.0952\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.8661 - accuracy: 0.2381 - val_loss: 3.3241 - val_accuracy: 0.0952\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.8483 - accuracy: 0.2381 - val_loss: 3.3131 - val_accuracy: 0.0952\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.8090 - accuracy: 0.2381 - val_loss: 3.3004 - val_accuracy: 0.0952\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.7658 - accuracy: 0.2381 - val_loss: 3.2898 - val_accuracy: 0.0952\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.7725 - accuracy: 0.2381 - val_loss: 3.2800 - val_accuracy: 0.0952\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.7450 - accuracy: 0.2381 - val_loss: 3.2646 - val_accuracy: 0.0952\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.7206 - accuracy: 0.2738 - val_loss: 3.2436 - val_accuracy: 0.0952\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.6623 - accuracy: 0.2857 - val_loss: 3.2230 - val_accuracy: 0.0952\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.6336 - accuracy: 0.3214 - val_loss: 3.2055 - val_accuracy: 0.0952\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.6348 - accuracy: 0.3214 - val_loss: 3.1858 - val_accuracy: 0.0952\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.5872 - accuracy: 0.2976 - val_loss: 3.1598 - val_accuracy: 0.0952\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.5607 - accuracy: 0.3333 - val_loss: 3.1321 - val_accuracy: 0.0952\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.5070 - accuracy: 0.3333 - val_loss: 3.1027 - val_accuracy: 0.0952\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.4630 - accuracy: 0.3095 - val_loss: 3.0763 - val_accuracy: 0.0952\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.4374 - accuracy: 0.3452 - val_loss: 3.0506 - val_accuracy: 0.0952\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.3669 - accuracy: 0.3690 - val_loss: 3.0147 - val_accuracy: 0.1429\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.3400 - accuracy: 0.3690 - val_loss: 2.9793 - val_accuracy: 0.1429\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.3373 - accuracy: 0.3571 - val_loss: 2.9612 - val_accuracy: 0.1429\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.2924 - accuracy: 0.3571 - val_loss: 2.9473 - val_accuracy: 0.1429\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.2459 - accuracy: 0.3690 - val_loss: 2.9154 - val_accuracy: 0.1429\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.1909 - accuracy: 0.4643 - val_loss: 2.8916 - val_accuracy: 0.1429\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.1759 - accuracy: 0.4405 - val_loss: 2.8507 - val_accuracy: 0.0952\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.1151 - accuracy: 0.4405 - val_loss: 2.7904 - val_accuracy: 0.1429\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.0731 - accuracy: 0.4762 - val_loss: 2.7380 - val_accuracy: 0.0952\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.0472 - accuracy: 0.4762 - val_loss: 2.6829 - val_accuracy: 0.1429\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.0111 - accuracy: 0.4524 - val_loss: 2.6523 - val_accuracy: 0.2381\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.9473 - accuracy: 0.5000 - val_loss: 2.6016 - val_accuracy: 0.0952\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.9472 - accuracy: 0.4762 - val_loss: 2.5664 - val_accuracy: 0.1429\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.9172 - accuracy: 0.4762 - val_loss: 2.5278 - val_accuracy: 0.1905\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.9053 - accuracy: 0.5119 - val_loss: 2.4758 - val_accuracy: 0.1429\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.8551 - accuracy: 0.5000 - val_loss: 2.4394 - val_accuracy: 0.2381\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.8158 - accuracy: 0.5952 - val_loss: 2.3900 - val_accuracy: 0.2381\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.8177 - accuracy: 0.5357 - val_loss: 2.3478 - val_accuracy: 0.1905\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.7601 - accuracy: 0.5833 - val_loss: 2.3019 - val_accuracy: 0.1905\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.6814 - accuracy: 0.5714 - val_loss: 2.2669 - val_accuracy: 0.1905\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.7497 - accuracy: 0.4881 - val_loss: 2.2563 - val_accuracy: 0.1905\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.7369 - accuracy: 0.5119 - val_loss: 2.2147 - val_accuracy: 0.1905\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.6754 - accuracy: 0.5357 - val_loss: 2.1881 - val_accuracy: 0.2857\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.6572 - accuracy: 0.5595 - val_loss: 2.1787 - val_accuracy: 0.2381\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.6595 - accuracy: 0.5595 - val_loss: 2.1813 - val_accuracy: 0.1905\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.6351 - accuracy: 0.5714 - val_loss: 2.2177 - val_accuracy: 0.1905\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.6356 - accuracy: 0.5714 - val_loss: 2.2468 - val_accuracy: 0.1905\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.6032 - accuracy: 0.6071 - val_loss: 2.2208 - val_accuracy: 0.1905\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.5136 - accuracy: 0.6190 - val_loss: 2.1381 - val_accuracy: 0.1905\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.5340 - accuracy: 0.5595 - val_loss: 2.0332 - val_accuracy: 0.1905\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.5442 - accuracy: 0.6071 - val_loss: 1.9845 - val_accuracy: 0.1905\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.5145 - accuracy: 0.6548 - val_loss: 1.9727 - val_accuracy: 0.2381\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.4931 - accuracy: 0.6548 - val_loss: 2.0616 - val_accuracy: 0.2381\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.4539 - accuracy: 0.6548 - val_loss: 2.1225 - val_accuracy: 0.2381\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.4808 - accuracy: 0.5952 - val_loss: 2.0970 - val_accuracy: 0.2857\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.5035 - accuracy: 0.5952 - val_loss: 2.0056 - val_accuracy: 0.3333\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.4831 - accuracy: 0.6548 - val_loss: 1.9171 - val_accuracy: 0.2857\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.4586 - accuracy: 0.6310 - val_loss: 1.9144 - val_accuracy: 0.2381\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.4203 - accuracy: 0.6190 - val_loss: 1.8369 - val_accuracy: 0.2857\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.3839 - accuracy: 0.6190 - val_loss: 1.8376 - val_accuracy: 0.3810\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.3748 - accuracy: 0.6786 - val_loss: 1.8915 - val_accuracy: 0.2857\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.3398 - accuracy: 0.6667 - val_loss: 1.8130 - val_accuracy: 0.3810\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.3375 - accuracy: 0.6786 - val_loss: 1.7801 - val_accuracy: 0.3810\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.2575 - accuracy: 0.7381 - val_loss: 1.7819 - val_accuracy: 0.3333\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.3020 - accuracy: 0.7024 - val_loss: 1.8135 - val_accuracy: 0.3333\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.2918 - accuracy: 0.6667 - val_loss: 1.7719 - val_accuracy: 0.3333\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.2709 - accuracy: 0.6905 - val_loss: 1.7164 - val_accuracy: 0.4762\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.2023 - accuracy: 0.7381 - val_loss: 1.6892 - val_accuracy: 0.5238\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.2017 - accuracy: 0.7381 - val_loss: 1.6836 - val_accuracy: 0.4286\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.2092 - accuracy: 0.7857 - val_loss: 1.6478 - val_accuracy: 0.5714\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1943 - accuracy: 0.7619 - val_loss: 1.6045 - val_accuracy: 0.5714\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.1580 - accuracy: 0.7500 - val_loss: 1.5820 - val_accuracy: 0.5714\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1216 - accuracy: 0.8214 - val_loss: 1.5864 - val_accuracy: 0.6667\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1355 - accuracy: 0.7262 - val_loss: 1.5679 - val_accuracy: 0.6667\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1733 - accuracy: 0.7619 - val_loss: 1.5215 - val_accuracy: 0.5714\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1304 - accuracy: 0.7619 - val_loss: 1.5042 - val_accuracy: 0.5238\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.1495 - accuracy: 0.7262 - val_loss: 1.4791 - val_accuracy: 0.6667\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0610 - accuracy: 0.7857 - val_loss: 1.5259 - val_accuracy: 0.6667\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.0238 - accuracy: 0.8333 - val_loss: 1.5225 - val_accuracy: 0.6667\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0681 - accuracy: 0.8452 - val_loss: 1.4356 - val_accuracy: 0.6667\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.0791 - accuracy: 0.8214 - val_loss: 1.4065 - val_accuracy: 0.6190\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.0553 - accuracy: 0.7619 - val_loss: 1.4415 - val_accuracy: 0.6190\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.9998 - accuracy: 0.8333 - val_loss: 1.5060 - val_accuracy: 0.6190\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.9869 - accuracy: 0.8571 - val_loss: 1.4687 - val_accuracy: 0.6190\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0136 - accuracy: 0.8810 - val_loss: 1.3154 - val_accuracy: 0.6190\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.0006 - accuracy: 0.8095 - val_loss: 1.3286 - val_accuracy: 0.5238\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "Training Time: 9.89 seconds\n",
      "Validation Accuracy: 0.5238\n",
      "Precision: 0.4881\n",
      "Recall: 0.5238\n",
      "F1 Score: 0.4952\n",
      "------------------------------\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 150ms/step - loss: 3.6239 - accuracy: 0.0595 - val_loss: 3.6096 - val_accuracy: 0.1429\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 3.5990 - accuracy: 0.1190 - val_loss: 3.5941 - val_accuracy: 0.1429\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 3.5790 - accuracy: 0.1429 - val_loss: 3.5780 - val_accuracy: 0.1429\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.5575 - accuracy: 0.1548 - val_loss: 3.5613 - val_accuracy: 0.1429\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.5314 - accuracy: 0.1071 - val_loss: 3.5429 - val_accuracy: 0.1429\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.5090 - accuracy: 0.1310 - val_loss: 3.5219 - val_accuracy: 0.1429\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.4744 - accuracy: 0.1429 - val_loss: 3.4973 - val_accuracy: 0.1429\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.4506 - accuracy: 0.1429 - val_loss: 3.4662 - val_accuracy: 0.1429\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.4092 - accuracy: 0.1429 - val_loss: 3.4293 - val_accuracy: 0.1429\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.3672 - accuracy: 0.1429 - val_loss: 3.3926 - val_accuracy: 0.1429\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.3429 - accuracy: 0.1429 - val_loss: 3.3576 - val_accuracy: 0.1429\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.2920 - accuracy: 0.1429 - val_loss: 3.3134 - val_accuracy: 0.1905\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.2537 - accuracy: 0.1429 - val_loss: 3.2610 - val_accuracy: 0.2381\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.2210 - accuracy: 0.1548 - val_loss: 3.1988 - val_accuracy: 0.2381\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.1851 - accuracy: 0.1905 - val_loss: 3.1241 - val_accuracy: 0.2857\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.1317 - accuracy: 0.1905 - val_loss: 3.0440 - val_accuracy: 0.3333\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.0580 - accuracy: 0.2262 - val_loss: 2.9640 - val_accuracy: 0.2381\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.0112 - accuracy: 0.2381 - val_loss: 2.8900 - val_accuracy: 0.2381\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.9568 - accuracy: 0.1905 - val_loss: 2.8351 - val_accuracy: 0.2381\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.9199 - accuracy: 0.2024 - val_loss: 2.7990 - val_accuracy: 0.2381\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.9217 - accuracy: 0.1905 - val_loss: 2.7765 - val_accuracy: 0.2381\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.8824 - accuracy: 0.2143 - val_loss: 2.7666 - val_accuracy: 0.2381\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.8794 - accuracy: 0.2024 - val_loss: 2.7689 - val_accuracy: 0.2381\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.8078 - accuracy: 0.2262 - val_loss: 2.7684 - val_accuracy: 0.2381\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.7973 - accuracy: 0.2381 - val_loss: 2.7420 - val_accuracy: 0.2381\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.7562 - accuracy: 0.3095 - val_loss: 2.7005 - val_accuracy: 0.2381\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.6966 - accuracy: 0.3333 - val_loss: 2.6579 - val_accuracy: 0.2381\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.6186 - accuracy: 0.3214 - val_loss: 2.6215 - val_accuracy: 0.3333\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.6157 - accuracy: 0.2857 - val_loss: 2.5875 - val_accuracy: 0.3333\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.5641 - accuracy: 0.3214 - val_loss: 2.5534 - val_accuracy: 0.3333\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.5436 - accuracy: 0.3452 - val_loss: 2.5247 - val_accuracy: 0.3333\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.4829 - accuracy: 0.3214 - val_loss: 2.4923 - val_accuracy: 0.3333\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.4168 - accuracy: 0.3810 - val_loss: 2.4560 - val_accuracy: 0.3333\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.3949 - accuracy: 0.3452 - val_loss: 2.4159 - val_accuracy: 0.2857\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.3681 - accuracy: 0.4048 - val_loss: 2.3874 - val_accuracy: 0.2381\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.2832 - accuracy: 0.3214 - val_loss: 2.3660 - val_accuracy: 0.2381\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.2483 - accuracy: 0.3690 - val_loss: 2.3406 - val_accuracy: 0.2381\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.2155 - accuracy: 0.4643 - val_loss: 2.3100 - val_accuracy: 0.2857\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.1244 - accuracy: 0.5119 - val_loss: 2.2700 - val_accuracy: 0.3333\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.1621 - accuracy: 0.4405 - val_loss: 2.2444 - val_accuracy: 0.3333\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.0655 - accuracy: 0.5000 - val_loss: 2.2445 - val_accuracy: 0.3333\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.0816 - accuracy: 0.4643 - val_loss: 2.1888 - val_accuracy: 0.3333\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.0574 - accuracy: 0.4762 - val_loss: 2.1445 - val_accuracy: 0.3333\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.0414 - accuracy: 0.4881 - val_loss: 2.0962 - val_accuracy: 0.3333\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.0148 - accuracy: 0.4286 - val_loss: 2.1284 - val_accuracy: 0.3810\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.8937 - accuracy: 0.5238 - val_loss: 2.0806 - val_accuracy: 0.3810\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.9173 - accuracy: 0.4881 - val_loss: 1.9983 - val_accuracy: 0.4286\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.8883 - accuracy: 0.4881 - val_loss: 1.9757 - val_accuracy: 0.4286\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.8283 - accuracy: 0.5952 - val_loss: 1.9685 - val_accuracy: 0.4762\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.8618 - accuracy: 0.4524 - val_loss: 2.0294 - val_accuracy: 0.3333\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.8115 - accuracy: 0.5476 - val_loss: 1.9807 - val_accuracy: 0.3810\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.7751 - accuracy: 0.5476 - val_loss: 1.8678 - val_accuracy: 0.4286\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.7677 - accuracy: 0.5833 - val_loss: 1.8440 - val_accuracy: 0.5238\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.7282 - accuracy: 0.5357 - val_loss: 1.8353 - val_accuracy: 0.5238\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.7284 - accuracy: 0.5119 - val_loss: 1.8267 - val_accuracy: 0.4762\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.6958 - accuracy: 0.5595 - val_loss: 1.7965 - val_accuracy: 0.4762\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.6518 - accuracy: 0.5714 - val_loss: 1.7729 - val_accuracy: 0.5238\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.6465 - accuracy: 0.6429 - val_loss: 1.7616 - val_accuracy: 0.5238\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.6516 - accuracy: 0.5714 - val_loss: 1.7496 - val_accuracy: 0.5714\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.5823 - accuracy: 0.6190 - val_loss: 1.7348 - val_accuracy: 0.5714\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.6180 - accuracy: 0.6190 - val_loss: 1.7068 - val_accuracy: 0.5714\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.5744 - accuracy: 0.6310 - val_loss: 1.6611 - val_accuracy: 0.5714\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.4535 - accuracy: 0.7381 - val_loss: 1.6372 - val_accuracy: 0.5714\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.4925 - accuracy: 0.7262 - val_loss: 1.6283 - val_accuracy: 0.5714\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.5108 - accuracy: 0.6190 - val_loss: 1.5856 - val_accuracy: 0.6190\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.4441 - accuracy: 0.7619 - val_loss: 1.5346 - val_accuracy: 0.5714\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.3770 - accuracy: 0.7262 - val_loss: 1.5086 - val_accuracy: 0.6190\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.3969 - accuracy: 0.7500 - val_loss: 1.4650 - val_accuracy: 0.6190\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.3620 - accuracy: 0.6786 - val_loss: 1.4350 - val_accuracy: 0.6190\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.3438 - accuracy: 0.7619 - val_loss: 1.3947 - val_accuracy: 0.6667\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.3146 - accuracy: 0.7738 - val_loss: 1.3522 - val_accuracy: 0.7143\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.2640 - accuracy: 0.7619 - val_loss: 1.3102 - val_accuracy: 0.7619\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.2670 - accuracy: 0.8095 - val_loss: 1.2687 - val_accuracy: 0.8095\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.2640 - accuracy: 0.7619 - val_loss: 1.2542 - val_accuracy: 0.7143\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1726 - accuracy: 0.7857 - val_loss: 1.2537 - val_accuracy: 0.7619\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1682 - accuracy: 0.8452 - val_loss: 1.2311 - val_accuracy: 0.7619\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.1386 - accuracy: 0.7738 - val_loss: 1.2123 - val_accuracy: 0.8095\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1541 - accuracy: 0.7738 - val_loss: 1.1962 - val_accuracy: 0.8095\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0965 - accuracy: 0.8690 - val_loss: 1.1720 - val_accuracy: 0.7619\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.0798 - accuracy: 0.8452 - val_loss: 1.1572 - val_accuracy: 0.7619\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.0061 - accuracy: 0.8810 - val_loss: 1.1269 - val_accuracy: 0.6667\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0407 - accuracy: 0.8929 - val_loss: 1.1054 - val_accuracy: 0.7619\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.0227 - accuracy: 0.8571 - val_loss: 1.1138 - val_accuracy: 0.8095\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.9842 - accuracy: 0.8452 - val_loss: 1.1361 - val_accuracy: 0.8095\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0510 - accuracy: 0.8214 - val_loss: 1.1085 - val_accuracy: 0.8571\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.9615 - accuracy: 0.9048 - val_loss: 1.0686 - val_accuracy: 0.8095\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.9239 - accuracy: 0.9048 - val_loss: 1.0624 - val_accuracy: 0.8095\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.9064 - accuracy: 0.9167 - val_loss: 1.0549 - val_accuracy: 0.8571\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.9252 - accuracy: 0.9167 - val_loss: 1.0243 - val_accuracy: 0.8571\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.8911 - accuracy: 0.9048 - val_loss: 1.0031 - val_accuracy: 0.8095\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.9140 - accuracy: 0.8690 - val_loss: 0.9826 - val_accuracy: 0.8095\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.8915 - accuracy: 0.9286 - val_loss: 0.9568 - val_accuracy: 0.8571\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.9106 - accuracy: 0.8690 - val_loss: 0.9210 - val_accuracy: 0.9048\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.8665 - accuracy: 0.9167 - val_loss: 0.9001 - val_accuracy: 0.9048\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.8413 - accuracy: 0.8810 - val_loss: 0.9013 - val_accuracy: 0.9048\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.8389 - accuracy: 0.8690 - val_loss: 0.9155 - val_accuracy: 0.8571\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.8468 - accuracy: 0.8929 - val_loss: 0.9229 - val_accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.8095 - accuracy: 0.9167 - val_loss: 0.9024 - val_accuracy: 0.9524\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8564 - accuracy: 0.9167 - val_loss: 0.8400 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7716 - accuracy: 0.9405 - val_loss: 0.8214 - val_accuracy: 1.0000\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3173b74c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3173b74c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step\n",
      "Training Time: 9.92 seconds\n",
      "Validation Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "------------------------------\n",
      "\n",
      "Average Metrics Over All Folds:\n",
      "Average Training Time: 9.95 seconds\n",
      "Average Validation Accuracy: 0.9048\n",
      "Average Precision: 0.8976\n",
      "Average Recall: 0.9048\n",
      "Average F1 Score: 0.8990\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "training_times = []\n",
    "val_accuracies = []\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "for train_index, val_index in kf.split(padded_sequences):\n",
    "    print(f\"Fold {fold}\")\n",
    "    X_train, X_val = padded_sequences[train_index], padded_sequences[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    model = create_model(input_length=padded_sequences.shape[1], vocab_size=len(word_index)+1, num_classes=len(label_encoder.classes_),\n",
    "                         embedding_dim=128, lstm_units=128, dense_units=64, dropout_rate=0.4, learning_rate=0.0005, \n",
    "                         lstm_dropout=0.3, recurrent_dropout=0.3, optimizer_choice='Adam', activation='tanh', regularizer=l2(0.01))\n",
    "    \n",
    "    # Train the model and measure training time\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n",
    "    training_time = time.time() - start_time\n",
    "    training_times.append(training_time)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred_classes)\n",
    "    val_precision = precision_score(y_val, y_val_pred_classes, average='weighted')\n",
    "    val_recall = recall_score(y_val, y_val_pred_classes, average='weighted')\n",
    "    val_f1 = f1_score(y_val, y_val_pred_classes, average='weighted')\n",
    "    \n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f'Training Time: {training_time:.2f} seconds')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'Precision: {val_precision:.4f}')\n",
    "    print(f'Recall: {val_recall:.4f}')\n",
    "    print(f'F1 Score: {val_f1:.4f}')\n",
    "    print(\"-\" * 30)\n",
    "    fold += 1\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics Over All Folds:\")\n",
    "print(f'Average Training Time: {np.mean(training_times):.2f} seconds')\n",
    "print(f'Average Validation Accuracy: {np.mean(val_accuracies):.4f}')\n",
    "print(f'Average Precision: {np.mean(val_precisions):.4f}')\n",
    "print(f'Average Recall: {np.mean(val_recalls):.4f}')\n",
    "print(f'Average F1 Score: {np.mean(val_f1s):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c128c4d-7351-485a-bbe7-d5e7ce26dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'quit' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "Bot: Hey! How can I help?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "Bot: An academic appeal is a request from a student to the Dean of their School to review a decision made by the Board of Examiners about an assessment decision. If you want tomake an appeal [https://ask.herts.ac.uk/academic-appeals-requests-for-the-review-of-assessment-decisions], you must request a review of your results within 10 working days of their publication. You can also contact your programme leader or cohort leader, or Herts SUs Advice and Support centre for guidance.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  when can i appeal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "Bot: An academic appeal is a request from a student to the Dean of their School to review a decision made by the Board of Examiners about an assessment decision. If you want tomake an appeal [https://ask.herts.ac.uk/academic-appeals-requests-for-the-review-of-assessment-decisions], you must request a review of your results within 10 working days of their publication. You can also contact your programme leader or cohort leader, or Herts SUs Advice and Support centre for guidance.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  when can i appal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "Bot: Herts SUs Advice and Support team [https://hertssu.com/your-support/]provides free, confidential, and impartial advice and support on a range of student issues, including those of an academic nature, such as information on academic appeals and academic misconduct through to study skills support. If you want to get in touch with the team for advice regarding your academic experience, you canbook an appointment. [https://hertssu.com/your-support/] Herts SU also runsStudy Smart [https://hertssu.com/your-support/academic-advice/study-smart/], a scheme which is designed to accommodate your unique learning preferences to make you feel more confident with your assignments, revision techniques and exams. Sessions are one-to-one tutorials where you'll be exploring new study strategies that are designed to lead to efficient and interesting ways that you can use to approach your assignments and deadlines.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is student wllbeing support\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "Bot: Good day! What do you need help with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is future of ai \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n",
      "Bot: Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is future of ai tools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n",
      "Bot: The only occasions where you may use AI tools in your assessment is if you have explicit permission from your tutor in your assessment brief. Your assessment brief will include information on how to declare any use of such tools, and you can speak to your tutor for guidance. If you do not reference your use, then this will constitute academic misconduct. Our current University policy on academic misconduct adequately covers the misuse of such tools, but we are updating them to be clearer on the matter. Unauthorised use of artificially generated material (AI) in researching or presenting material for an assessment is an academic misconduct offence if you use AI tools in producing your assessment unless the use of AI tools is expressly permitted. However, even if expressly permitted, where you do not declare that you have used an artificial intelligence tool(s) in the production of your assessment, or you are dishonest about the extent to which such tools have been used, you will have committed academic misconduct.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "def get_response(user_input):\n",
    "    user_input = preprocess(user_input)\n",
    "    seq = tokenizer.texts_to_sequences([user_input])\n",
    "    padded_seq = pad_sequences(seq, maxlen=padded_sequences.shape[1], padding='post')\n",
    "    pred = model.predict(padded_seq)\n",
    "    tag_index = np.argmax(pred)\n",
    "    tag = label_encoder.inverse_transform([tag_index])[0]\n",
    "\n",
    "    if max(pred[0]) < 0.6:  # Adjust threshold for confidence\n",
    "        user_vector = response_vectorizer.transform([user_input])\n",
    "        similarity_scores = cosine_similarity(user_vector, response_vectors)\n",
    "        best_match_index = np.argmax(similarity_scores)\n",
    "        best_match_response = all_responses[best_match_index]\n",
    "        \n",
    "        for intent in data['intents']:\n",
    "            if best_match_response in intent['responses']:\n",
    "                tag = intent['tag']\n",
    "                break\n",
    "\n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            return random.choice(intent['responses'])\n",
    "\n",
    "print(\"Chatbot is ready! Type 'quit' to exit.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = get_response(user_input)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d11b8-959e-4912-8d7b-88261bcc8bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659ad30-fbb5-4479-b5e0-2bbddd2a7504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c97e8e-cb11-4cf1-96eb-c9993f88ff16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for the plot\n",
    "optimizers = ['Adam', 'RMSprop', 'SGD', 'Adamax']\n",
    "activations = ['relu', 'tanh', 'sigmoid']\n",
    "accuracies = {\n",
    "    'relu': [0.8000, 0.4762, 0.1048, 0.2000],\n",
    "    'tanh': [0.8571, 0.7524, 0.0952, 0.3048],\n",
    "    'sigmoid': [0.3810, 0.3238, 0.0571, 0.1714]\n",
    "}\n",
    "\n",
    "# Plotting the grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Set width of bars\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(optimizers))\n",
    "\n",
    "# Plot each activation function's data\n",
    "for i, (activation, accs) in enumerate(accuracies.items()):\n",
    "    plt.bar(index + i * bar_width, accs, bar_width, label=f'{activation}')\n",
    "\n",
    "# Customization\n",
    "plt.xlabel('Optimizers')\n",
    "plt.ylabel('Average Validation Accuracy')\n",
    "plt.title('Average Validation Accuracy by Optimizer and Activation Function')\n",
    "plt.xticks(index + bar_width, optimizers)\n",
    "plt.ylim(0, 1)  # Set y-axis limit from 0 to 1\n",
    "plt.legend(title='Activation Function')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Adding the accuracy values on top of the bars\n",
    "for i, (activation, accs) in enumerate(accuracies.items()):\n",
    "    for j, acc in enumerate(accs):\n",
    "        plt.text(j + i * bar_width, acc + 0.02, f\"{acc:.2f}\", ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('LSTM.png', format='png')  # Save as PNG file\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6494ff9-fe8a-4a04-8d55-c679cb69f843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
